"""
ê°•í™”í•™ìŠµ & ì˜ì‚¬ê²°ì • ìµœì í™” - AI ê¸°ë°˜ ì„ íƒ

ì—­í• :
- ìƒí™©ë³„ ìµœì  ì„ íƒ
- ë³´ìƒ ê¸°ë°˜ í•™ìŠµ
- ì •ì±… ìµœì í™”
- ì ì‘í˜• ì˜ì‚¬ê²°ì •
"""

from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
import random


@dataclass
class State:
    """ìƒíƒœ"""
    id: str
    features: Dict[str, float]
    reward: float = 0.0


@dataclass
class Action:
    """í–‰ë™"""
    id: str
    name: str
    estimated_reward: float = 0.0
    times_taken: int = 0
    success_count: int = 0


class MultiArmedBandit:
    """ë‹¤ì¤‘ íŒ” ìŠ¬ë¡¯ ë¨¸ì‹  (ê¸°ë³¸ ê°•í™”í•™ìŠµ)"""
    
    def __init__(self, epsilon: float = 0.1):
        self.epsilon = epsilon  # íƒí—˜ë¥ 
        self.actions: Dict[str, Action] = {}
        self.history = []
    
    def add_action(self, action: Action):
        """í–‰ë™ ì¶”ê°€"""
        self.actions[action.id] = action
    
    def select_action(self) -> Action:
        """í–‰ë™ ì„ íƒ (Îµ-ê·¸ë¦¬ë””)"""
        
        if random.random() < self.epsilon:
            # íƒí—˜: ë¬´ì‘ìœ„ ì„ íƒ
            return random.choice(list(self.actions.values()))
        else:
            # í™œìš©: ìµœê³  ë³´ìƒ ì„ íƒ
            return max(self.actions.values(), key=lambda a: a.estimated_reward)
    
    def update_action(self, action_id: str, reward: float):
        """í–‰ë™ ê²°ê³¼ ì—…ë°ì´íŠ¸"""
        
        if action_id not in self.actions:
            return
        
        action = self.actions[action_id]
        action.times_taken += 1
        
        if reward > 0:
            action.success_count += 1
        
        # í‰ê·  ë³´ìƒ ì—…ë°ì´íŠ¸
        action.estimated_reward = action.success_count / action.times_taken
        
        self.history.append({
            'action_id': action_id,
            'reward': reward,
            'cumulative_reward': sum(a.estimated_reward * a.times_taken 
                                    for a in self.actions.values())
        })


class QLearning:
    """Q-ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜"""
    
    def __init__(self, learning_rate: float = 0.1, discount_factor: float = 0.9):
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.q_table: Dict[Tuple[str, str], float] = {}  # (state, action) -> Q-value
    
    def get_q_value(self, state_id: str, action_id: str) -> float:
        """Q-ê°’ ì¡°íšŒ"""
        return self.q_table.get((state_id, action_id), 0.0)
    
    def update_q_value(
        self,
        state_id: str,
        action_id: str,
        reward: float,
        next_state_id: str,
        next_actions: List[str]
    ):
        """Q-ê°’ ì—…ë°ì´íŠ¸"""
        
        current_q = self.get_q_value(state_id, action_id)
        
        # ë‹¤ìŒ ìƒíƒœì—ì„œ ìµœê³  Q-ê°’ ì¡°íšŒ
        max_next_q = max(
            [self.get_q_value(next_state_id, a) for a in next_actions],
            default=0.0
        )
        
        # Q-ê°’ ì—…ë°ì´íŠ¸
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[(state_id, action_id)] = new_q
    
    def select_best_action(self, state_id: str, actions: List[str]) -> str:
        """ìµœê³  í–‰ë™ ì„ íƒ"""
        
        q_values = [self.get_q_value(state_id, a) for a in actions]
        best_action_idx = q_values.index(max(q_values))
        return actions[best_action_idx]


class PolicyOptimizer:
    """ì •ì±… ìµœì í™”"""
    
    def __init__(self):
        self.q_learning = QLearning()
        self.multi_armed_bandit = MultiArmedBandit(epsilon=0.15)
        self.policy_history = []
    
    async def optimize_decision(
        self,
        current_state: State,
        available_actions: List[Action],
        context: Dict[str, Any] = None
    ) -> Tuple[Action, Dict[str, Any]]:
        """ì˜ì‚¬ê²°ì • ìµœì í™”"""
        
        decision_data = {
            'state_id': current_state.id,
            'state_features': current_state.features,
            'available_actions': [a.name for a in available_actions],
            'context': context or {}
        }
        
        # Q-ëŸ¬ë‹ ê¸°ë°˜ ì„ íƒ
        if available_actions:
            action_ids = [a.id for a in available_actions]
            best_action_id = self.q_learning.select_best_action(
                current_state.id,
                action_ids
            )
            selected_action = next(a for a in available_actions if a.id == best_action_id)
        else:
            selected_action = available_actions[0] if available_actions else None
        
        decision_data['selected_action'] = selected_action.name if selected_action else None
        decision_data['q_value'] = self.q_learning.get_q_value(
            current_state.id,
            selected_action.id
        ) if selected_action else 0.0
        
        self.policy_history.append(decision_data)
        
        return selected_action, decision_data
    
    def learn_from_feedback(
        self,
        state_id: str,
        action_id: str,
        reward: float,
        next_state_id: str,
        next_actions: List[str]
    ):
        """í”¼ë“œë°±ìœ¼ë¡œë¶€í„° í•™ìŠµ"""
        
        self.q_learning.update_q_value(
            state_id,
            action_id,
            reward,
            next_state_id,
            next_actions
        )
        
        self.multi_armed_bandit.update_action(action_id, reward)
    
    def get_policy_report(self) -> Dict[str, Any]:
        """ì •ì±… ë¦¬í¬íŠ¸"""
        
        return {
            'timestamp': __import__('datetime').datetime.now().isoformat(),
            'total_decisions': len(self.policy_history),
            'q_table_size': len(self.q_learning.q_table),
            'bandit_actions': len(self.multi_armed_bandit.actions),
            'recent_decisions': self.policy_history[-10:] if self.policy_history else [],
            'bandit_performance': {
                a.id: {
                    'times_taken': a.times_taken,
                    'success_count': a.success_count,
                    'success_rate': a.success_count / a.times_taken if a.times_taken > 0 else 0
                }
                for a in self.multi_armed_bandit.actions.values()
            }
        }


if __name__ == "__main__":
    print("ğŸ¤– ê°•í™”í•™ìŠµ & ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸\n")
    
    optimizer = PolicyOptimizer()
    
    # í–‰ë™ ì„¤ì •
    actions = [
        Action("action_1", "aggressive_strategy", 0.5),
        Action("action_2", "conservative_strategy", 0.3),
        Action("action_3", "balanced_strategy", 0.6),
    ]
    
    for action in actions:
        optimizer.multi_armed_bandit.add_action(action)
    
    # ì‹œë®¬ë ˆì´ì…˜
    state = State("market_state", {"volatility": 0.8, "trend": 0.6})
    
    import asyncio
    
    async def test():
        # 10ë²ˆ ë°˜ë³µ
        for i in range(10):
            action, decision = await optimizer.optimize_decision(state, actions)
            
            # ë³´ìƒ ì‹œë®¬ë ˆì´ì…˜
            reward = 1.0 if i % 2 == 0 else 0.5
            
            # í•™ìŠµ
            optimizer.learn_from_feedback(
                state.id,
                action.id,
                reward,
                "next_state",
                [a.id for a in actions]
            )
            
            print(f"Step {i+1}: {action.name} -> Reward: {reward}")
        
        # ë¦¬í¬íŠ¸
        report = optimizer.get_policy_report()
        print(f"\nâœ… ì •ì±… í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì˜ì‚¬ê²°ì •: {report['total_decisions']}")
        print(f"Q-í…Œì´ë¸” í¬ê¸°: {report['q_table_size']}")
    
    asyncio.run(test())
