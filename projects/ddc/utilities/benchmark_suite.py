#!/usr/bin/env python3
"""
D-CNS Performance Benchmark Suite (v1.0)
í•™ìˆ  ë…¼ë¬¸ ë° ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì •ëŸ‰ì  ë°ì´í„° ìˆ˜ì§‘ ë„êµ¬
"""

import os
import sys
import time
import json
import asyncio
import statistics
from datetime import datetime
from typing import Dict, List, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(os.getcwd())

try:
    from projects.ddc.brain.brain_core.chat_engine import get_chat_engine
except ImportError:
    print("âŒ í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì • ì˜¤ë¥˜. Rootì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

class BenchmarkSuite:
    def __init__(self):
        self.engine = get_chat_engine()
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "environment": "Local (MacBook)",
            "layers": {}
        }
        
    async def measure_latency(self, layer: str, model_id: str, prompt: str, trials: int = 3) -> Dict:
        """ë‹¨ì¼ ëª¨ë¸ Latency ë° ì„±ê³µë¥  ì¸¡ì •"""
        print(f"\nğŸ§ª Testing {layer} [{model_id}] ({trials} trials)...")
        latencies = []
        errors = 0
        
        for i in range(trials):
            start_time = time.time()
            try:
                # ChatEngineì˜ ë‚´ë¶€ ë©”ì„œë“œë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ê±°ë‚˜ get_response í˜¸ì¶œ
                # ì—¬ê¸°ì„œëŠ” ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸ë¥¼ ê²¸í•´ get_response ì‚¬ìš©í•˜ë˜, íŠ¹ì • í‚¤ì›Œë“œë¡œ ìœ ë„
                response = await self.engine.get_response(999999, prompt)
                duration = (time.time() - start_time) * 1000 # ms
                
                if "âš ï¸" in response or "Error" in response:
                    print(f"   Trial {i+1}: âŒ Failed ({duration:.1f}ms)")
                    errors += 1
                else:
                    print(f"   Trial {i+1}: âœ… {duration:.1f}ms")
                    latencies.append(duration)
                    
            except Exception as e:
                print(f"   Trial {i+1}: ğŸ’¥ Exception: {e}")
                errors += 1
            
            # Rate Limit ë°©ì§€
            await asyncio.sleep(1)

        if not latencies:
            return {"avg_ms": 0, "p99_ms": 0, "success_rate": 0.0}

        return {
            "model": model_id,
            "samples": trials,
            "avg_ms": round(statistics.mean(latencies), 2),
            "min_ms": round(min(latencies), 2),
            "max_ms": round(max(latencies), 2),
            "p99_ms": round(statistics.quantiles(latencies, n=100)[98] if len(latencies) > 1 else max(latencies), 2),
            "success_rate": round((trials - errors) / trials * 100, 1)
        }

    async def run_full_benchmark(self):
        """L1-L4 ì „ì²´ ê³„ì¸µ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰"""
        print("="*60)
        print("ğŸš€ D-CNS Neural Benchmark Started")
        print("="*60)

        # Test Cases (ê° ë ˆì´ì–´ë¥¼ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸)
        scenarios = [
            ("L1", "L1 Reflexive", "ì•ˆë…•! ë¹¨ë¦¬ ëŒ€ë‹µí•´ì¤˜. (Speed Test)"),
            ("L2", "L2 Affective", "ì˜¤ëŠ˜ ì—°êµ¬ê°€ ë„ˆë¬´ í˜ë“¤ì–´ì„œ ì¢€ ìš°ìš¸í•´. (Emotion Test)"),
            ("L3", "L3 Cognitive", "D-CNS ì•„í‚¤í…ì²˜ì˜ ì¥ì ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ì„í•´ì¤˜. (Logic Test)"),
            ("L4", "L4 NeuroNet", "Pythonìœ¼ë¡œ ìƒëª…ê²Œì„ êµ¬í˜„í•˜ëŠ” ì½”ë“œ ì§œì¤˜. (Coding Test)")
        ]

        total_start = time.time()

        for layer_code, layer_name, prompt in scenarios:
            # í•´ë‹¹ ë ˆì´ì–´ì˜ ëŒ€í‘œ ëª¨ë¸ (ChatEngine ë¡œì§ ì°¸ì¡°)
            # ë™ì  ë¼ìš°íŒ…ì´ë¯€ë¡œ ì‹¤ì œ í˜¸ì¶œëœ ëª¨ë¸ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ -> ë¡œê·¸ í™•ì¸ í•„ìš”í•˜ì§€ë§Œ
            # ì—¬ê¸°ì„œëŠ” ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ì— ì§‘ì¤‘
            
            result = await self.measure_latency(layer_code, "Auto-Routed", prompt, trials=3)
            self.results["layers"][layer_name] = result

        total_duration = time.time() - total_start
        self.results["total_duration_sec"] = round(total_duration, 2)
        
        self.save_report()

    def save_report(self):
        """ê²°ê³¼ ë ˆí¬íŠ¸ ì €ì¥"""
        filename = f"docs/reports/benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # ë””ë ‰í† ë¦¬ í™•ì¸
        os.makedirs("docs/reports", exist_ok=True)
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
            
        print("\n" + "="*60)
        print(f"ğŸ’¾ Benchmark Report Saved: {filename}")
        print("="*60)
        
        # ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“Š Summary (Environment: {self.results['environment']})")
        for layer, data in self.results["layers"].items():
            status = "ğŸŸ¢" if data['success_rate'] > 90 else "ğŸ”´"
            print(f"{status} {layer}: {data['avg_ms']}ms (Success: {data['success_rate']}%)")

if __name__ == "__main__":
    benchmark = BenchmarkSuite()
    asyncio.run(benchmark.run_full_benchmark())
