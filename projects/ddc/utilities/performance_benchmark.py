"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ - v5.1.0 vs v5.2.0

ëª©í‘œ:
- ê¸°ì¡´ ì„±ëŠ¥ ì¸¡ì •
- ìµœì í™” íš¨ê³¼ ê²€ì¦
- ë³‘ëª© ì§€ì  íŒŒì•…
"""

import time
import asyncio
from typing import Callable, Dict, List, Any
import statistics


class BenchmarkRunner:
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.results = {}
    
    def run_benchmark(
        self,
        name: str,
        func: Callable,
        iterations: int = 100,
        *args,
        **kwargs
    ) -> Dict[str, float]:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        times = []
        
        for _ in range(iterations):
            start = time.perf_counter()
            func(*args, **kwargs)
            end = time.perf_counter()
            times.append((end - start) * 1000)  # ms
        
        stats = {
            'iterations': iterations,
            'mean_ms': statistics.mean(times),
            'median_ms': statistics.median(times),
            'stdev_ms': statistics.stdev(times) if len(times) > 1 else 0,
            'min_ms': min(times),
            'max_ms': max(times),
            'p95_ms': sorted(times)[int(len(times) * 0.95)],
            'p99_ms': sorted(times)[int(len(times) * 0.99)],
            'throughput': iterations / (sum(times) / 1000)  # ops/sec
        }
        
        self.results[name] = stats
        return stats
    
    async def run_async_benchmark(
        self,
        name: str,
        func: Callable,
        iterations: int = 100,
        *args,
        **kwargs
    ) -> Dict[str, float]:
        """ë¹„ë™ê¸° ë²¤ì¹˜ë§ˆí¬"""
        times = []
        
        for _ in range(iterations):
            start = time.perf_counter()
            await func(*args, **kwargs)
            end = time.perf_counter()
            times.append((end - start) * 1000)  # ms
        
        stats = {
            'iterations': iterations,
            'mean_ms': statistics.mean(times),
            'median_ms': statistics.median(times),
            'stdev_ms': statistics.stdev(times) if len(times) > 1 else 0,
            'min_ms': min(times),
            'max_ms': max(times),
            'p95_ms': sorted(times)[int(len(times) * 0.95)],
            'p99_ms': sorted(times)[int(len(times) * 0.99)],
            'throughput': iterations / (sum(times) / 1000)
        }
        
        self.results[name] = stats
        return stats
    
    def print_results(self):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘         ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼                      â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
        
        for name, stats in self.results.items():
            print(f"ğŸ“Œ {name}")
            print(f"  ë°˜ë³µ: {stats['iterations']}")
            print(f"  í‰ê· : {stats['mean_ms']:.2f}ms")
            print(f"  ì¤‘ì•™ê°’: {stats['median_ms']:.2f}ms")
            print(f"  í‘œì¤€í¸ì°¨: {stats['stdev_ms']:.2f}ms")
            print(f"  ìµœì†Œ: {stats['min_ms']:.2f}ms")
            print(f"  ìµœëŒ€: {stats['max_ms']:.2f}ms")
            print(f"  P95: {stats['p95_ms']:.2f}ms")
            print(f"  P99: {stats['p99_ms']:.2f}ms")
            print(f"  ì²˜ë¦¬ëŸ‰: {stats['throughput']:.0f} ops/sec")
            print()


class PerformanceComparison:
    """ì„±ëŠ¥ ë¹„êµ"""
    
    def __init__(self):
        self.v5_1_0 = {}  # ê¸°ì¡´ ì„±ëŠ¥
        self.v5_2_0 = {}  # ìµœì í™” í›„
    
    def compare(self, metric_name: str) -> Dict[str, float]:
        """ë¹„êµ ë¶„ì„"""
        v1 = self.v5_1_0.get(metric_name, 0)
        v2 = self.v5_2_0.get(metric_name, 0)
        
        if v1 == 0:
            return {}
        
        improvement = ((v1 - v2) / v1 * 100)
        ratio = v1 / v2 if v2 > 0 else 0
        
        return {
            'v5.1.0': v1,
            'v5.2.0': v2,
            'improvement_percent': round(improvement, 2),
            'speedup_ratio': round(ratio, 2),
            'status': 'âœ… ê°œì„ ' if improvement > 0 else 'âš ï¸ ì €í•˜'
        }
    
    def print_comparison(self):
        """ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
        print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘         ğŸ“ˆ v5.1.0 vs v5.2.0 ì„±ëŠ¥ ë¹„êµ              â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
        
        metrics = set(list(self.v5_1_0.keys()) + list(self.v5_2_0.keys()))
        
        for metric in metrics:
            result = self.compare(metric)
            if result:
                print(f"ğŸ“Š {metric}")
                print(f"  v5.1.0: {result['v5.1.0']:.2f}")
                print(f"  v5.2.0: {result['v5.2.0']:.2f}")
                print(f"  ê°œì„ ìœ¨: {result['improvement_percent']:.1f}%")
                print(f"  ì†ë„: {result['speedup_ratio']:.1f}ë°°")
                print(f"  ìƒíƒœ: {result['status']}")
                print()


class ProfileAnalyzer:
    """í”„ë¡œí•„ ë¶„ì„"""
    
    def __init__(self):
        self.call_times = {}
        self.call_counts = {}
    
    def profile(self, name: str):
        """í”„ë¡œí•„ ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start = time.perf_counter()
                result = func(*args, **kwargs)
                duration = (time.perf_counter() - start) * 1000
                
                if name not in self.call_times:
                    self.call_times[name] = []
                    self.call_counts[name] = 0
                
                self.call_times[name].append(duration)
                self.call_counts[name] += 1
                
                return result
            
            return wrapper
        return decorator
    
    def print_profile(self):
        """í”„ë¡œí•„ ì¶œë ¥"""
        print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘         ğŸ” í•¨ìˆ˜ë³„ ì„±ëŠ¥ í”„ë¡œí•„                      â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
        
        for name, times in sorted(
            self.call_times.items(),
            key=lambda x: sum(x[1]),
            reverse=True
        ):
            total = sum(times)
            count = self.call_counts[name]
            
            print(f"ğŸ“ {name}")
            print(f"  í˜¸ì¶œ: {count}íšŒ")
            print(f"  ì´ì‹œê°„: {total:.2f}ms")
            print(f"  í‰ê· : {total/count:.2f}ms")
            print(f"  ë¹„ìœ¨: {total/sum(sum(t for t in self.call_times.values()))*100:.1f}%")
            print()


# í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜ë“¤
def slow_operation():
    """ëŠë¦° ì‘ì—… (ì‹œë®¬ë ˆì´ì…˜)"""
    time.sleep(0.01)

def fast_operation():
    """ë¹ ë¥¸ ì‘ì—… (ìºì‹œë¨)"""
    time.sleep(0.001)

async def async_operation():
    """ë¹„ë™ê¸° ì‘ì—…"""
    await asyncio.sleep(0.005)


if __name__ == "__main__":
    print("ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = BenchmarkRunner()
    
    # ë™ê¸° ë²¤ì¹˜ë§ˆí¬
    benchmark.run_benchmark("ëŠë¦° ì‘ì—…", slow_operation, iterations=50)
    benchmark.run_benchmark("ë¹ ë¥¸ ì‘ì—… (ìºì‹œ)", fast_operation, iterations=100)
    
    benchmark.print_results()
    
    # ë¹„êµ ë¶„ì„
    comparison = PerformanceComparison()
    comparison.v5_1_0 = {'API ì‘ë‹µ': 100, 'ë©”ëª¨ë¦¬': 500}
    comparison.v5_2_0 = {'API ì‘ë‹µ': 50, 'ë©”ëª¨ë¦¬': 375}
    comparison.print_comparison()
    
    print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
