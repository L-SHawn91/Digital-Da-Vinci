"""
ë¶„ì‚° ì¶”ë¡  ì‹œìŠ¤í…œ - ë©€í‹° GPU/í´ë¼ìš°ë“œ ìµœì í™”

ì—­í• :
- ë¶„ì‚° ì—°ì‚° ê´€ë¦¬
- ë¡œë“œ ë°¸ëŸ°ì‹±
- ì¥ì•  ë³µêµ¬
- ë¹„ìš© ìµœì í™”
"""

from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass
from enum import Enum
import asyncio
from datetime import datetime


class ComputeNodeType(Enum):
    """ì»´í“¨íŠ¸ ë…¸ë“œ íƒ€ì…"""
    LOCAL_CPU = "local_cpu"
    LOCAL_GPU = "local_gpu"
    AWS_GPU = "aws_gpu"
    GOOGLE_TPU = "google_tpu"
    CLOUD_FUNCTION = "cloud_function"


@dataclass
class ComputeNode:
    """ì»´í“¨íŠ¸ ë…¸ë“œ"""
    id: str
    node_type: ComputeNodeType
    capacity: int  # ìµœëŒ€ ë™ì‹œ ì‘ì—…
    current_load: int  # í˜„ì¬ ì‘ì—… ìˆ˜
    latency_ms: float
    cost_per_hour: float
    availability: float  # 0-1
    available: bool = True
    
    def get_utilization(self) -> float:
        """ì‚¬ìš©ë¥ """
        return self.current_load / self.capacity if self.capacity > 0 else 0
    
    def get_cost_per_task(self, duration_ms: float) -> float:
        """ì‘ì—…ë‹¹ ë¹„ìš©"""
        return (self.cost_per_hour / 3600000) * duration_ms


class LoadBalancer:
    """ë¡œë“œ ë°¸ëŸ°ì„œ"""
    
    def __init__(self):
        self.nodes: Dict[str, ComputeNode] = {}
        self.task_history = []
    
    def register_node(self, node: ComputeNode):
        """ë…¸ë“œ ë“±ë¡"""
        self.nodes[node.id] = node
    
    def select_best_node(self, task_priority: str = 'balanced') -> Optional[ComputeNode]:
        """ìµœì  ë…¸ë“œ ì„ íƒ"""
        available_nodes = [
            n for n in self.nodes.values()
            if n.available and n.current_load < n.capacity
        ]
        
        if not available_nodes:
            return None
        
        if task_priority == 'speed':
            # ê°€ì¥ ë¹ ë¥¸ ë…¸ë“œ
            return min(available_nodes, key=lambda x: x.latency_ms)
        
        elif task_priority == 'cost':
            # ê°€ì¥ ì €ë ´í•œ ë…¸ë“œ
            return min(available_nodes, key=lambda x: x.cost_per_hour)
        
        else:  # balanced
            # ë¹„ìš©-ì„±ëŠ¥ ê· í˜•
            return min(
                available_nodes,
                key=lambda x: (x.latency_ms / 1000) / (x.cost_per_hour + 0.001)
            )
    
    def distribute_load(self, tasks: List[Dict[str, Any]], priority: str = 'balanced') -> Dict[str, List]:
        """ì‘ì—… ë¶„ë°°"""
        distribution = {node_id: [] for node_id in self.nodes.keys()}
        
        for task in tasks:
            node = self.select_best_node(priority)
            if node:
                distribution[node.id].append(task)
                node.current_load += 1
                self.task_history.append({
                    'timestamp': datetime.now().isoformat(),
                    'task': task,
                    'node': node.id
                })
        
        return distribution


class DistributedInferenceSystem:
    """ë¶„ì‚° ì¶”ë¡  ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.load_balancer = LoadBalancer()
        self.cache = {}
        self.results = {}
        
        # ê¸°ë³¸ ë…¸ë“œ ì„¤ì •
        self._initialize_nodes()
    
    def _initialize_nodes(self):
        """ê¸°ë³¸ ë…¸ë“œ ì´ˆê¸°í™”"""
        nodes = [
            ComputeNode(
                id="local_cpu_1",
                node_type=ComputeNodeType.LOCAL_CPU,
                capacity=4,
                current_load=0,
                latency_ms=50,
                cost_per_hour=0,
                availability=0.99
            ),
            ComputeNode(
                id="local_gpu_1",
                node_type=ComputeNodeType.LOCAL_GPU,
                capacity=8,
                current_load=0,
                latency_ms=30,
                cost_per_hour=0.5,
                availability=0.95
            ),
            ComputeNode(
                id="aws_gpu_1",
                node_type=ComputeNodeType.AWS_GPU,
                capacity=16,
                current_load=0,
                latency_ms=150,
                cost_per_hour=2.5,
                availability=0.99
            ),
            ComputeNode(
                id="cloud_function_1",
                node_type=ComputeNodeType.CLOUD_FUNCTION,
                capacity=100,
                current_load=0,
                latency_ms=200,
                cost_per_hour=0.1,
                availability=0.98
            )
        ]
        
        for node in nodes:
            self.load_balancer.register_node(node)
    
    async def inference_on_node(
        self,
        node: ComputeNode,
        model: str,
        input_data: Any,
        timeout_ms: int = 5000
    ) -> Dict[str, Any]:
        """ë…¸ë“œì—ì„œ ì¶”ë¡  ì‹¤í–‰"""
        try:
            # ì‹œë®¬ë ˆì´ì…˜: ì‹¤ì œë¡œëŠ” ì›ê²© í˜¸ì¶œ
            await asyncio.sleep(node.latency_ms / 1000)
            
            result = {
                'model': model,
                'node': node.id,
                'latency_ms': node.latency_ms,
                'status': 'success',
                'result': f"Inference result from {node.id}",
                'timestamp': datetime.now().isoformat()
            }
            
            # ë¹„ìš© ê³„ì‚°
            result['cost'] = node.get_cost_per_task(node.latency_ms)
            
            return result
        
        except asyncio.TimeoutError:
            return {
                'status': 'timeout',
                'node': node.id,
                'error': 'Inference timeout'
            }
    
    async def distributed_inference(
        self,
        model: str,
        tasks: List[Any],
        strategy: str = 'balanced'
    ) -> Dict[str, Any]:
        """ë¶„ì‚° ì¶”ë¡ """
        
        # ì‘ì—… ìƒì„±
        task_list = [{'input': task, 'model': model} for task in tasks]
        
        # ë¡œë“œ ë°¸ëŸ°ì‹±
        distribution = self.load_balancer.distribute_load(task_list, strategy)
        
        # ë³‘ë ¬ ì‹¤í–‰
        inference_tasks = []
        for node_id, node_tasks in distribution.items():
            node = self.load_balancer.nodes[node_id]
            for task in node_tasks:
                inference_tasks.append(
                    self.inference_on_node(node, model, task['input'])
                )
        
        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        results = await asyncio.gather(*inference_tasks)
        
        # ê²°ê³¼ ë¶„ì„
        successful = sum(1 for r in results if r.get('status') == 'success')
        total_cost = sum(r.get('cost', 0) for r in results)
        avg_latency = sum(r.get('latency_ms', 0) for r in results) / len(results) if results else 0
        
        return {
            'timestamp': datetime.now().isoformat(),
            'model': model,
            'total_tasks': len(tasks),
            'successful': successful,
            'failed': len(tasks) - successful,
            'success_rate': successful / len(tasks) if tasks else 0,
            'total_cost': total_cost,
            'avg_latency_ms': avg_latency,
            'results': results
        }
    
    def get_node_stats(self) -> Dict[str, Any]:
        """ë…¸ë“œ í†µê³„"""
        stats = {}
        total_capacity = 0
        total_load = 0
        total_cost = 0
        
        for node_id, node in self.load_balancer.nodes.items():
            stats[node_id] = {
                'type': node.node_type.value,
                'utilization': node.get_utilization(),
                'current_load': node.current_load,
                'capacity': node.capacity,
                'latency_ms': node.latency_ms,
                'cost_per_hour': node.cost_per_hour,
                'availability': node.availability
            }
            
            total_capacity += node.capacity
            total_load += node.current_load
            total_cost += node.cost_per_hour
        
        return {
            'nodes': stats,
            'system_utilization': total_load / total_capacity if total_capacity > 0 else 0,
            'total_capacity': total_capacity,
            'total_load': total_load,
            'total_hourly_cost': total_cost
        }
    
    def optimize_configuration(self) -> Dict[str, Any]:
        """êµ¬ì„± ìµœì í™” ì œì•ˆ"""
        stats = self.get_node_stats()
        
        recommendations = []
        
        # í™œìš©ë„ ë¶„ì„
        for node_id, node_stats in stats['nodes'].items():
            utilization = node_stats['utilization']
            
            if utilization > 0.8:
                recommendations.append({
                    'node': node_id,
                    'action': 'scale_up',
                    'reason': f"High utilization: {utilization*100:.1f}%"
                })
            elif utilization < 0.2 and node_stats['cost_per_hour'] > 0:
                recommendations.append({
                    'node': node_id,
                    'action': 'scale_down',
                    'reason': f"Low utilization: {utilization*100:.1f}%"
                })
        
        return {
            'recommendations': recommendations,
            'current_stats': stats
        }


if __name__ == "__main__":
    print("ğŸ”„ ë¶„ì‚° ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n")
    
    system = DistributedInferenceSystem()
    
    # í…ŒìŠ¤íŠ¸ ì‘ì—…
    test_tasks = ["input_1", "input_2", "input_3", "input_4", "input_5"]
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    import asyncio
    result = asyncio.run(
        system.distributed_inference(
            "gemini_2_5_pro",
            test_tasks,
            strategy='balanced'
        )
    )
    
    print("âœ… ë¶„ì‚° ì¶”ë¡  ì™„ë£Œ!")
    print(f"ì„±ê³µ: {result['successful']}/{result['total_tasks']}")
    print(f"ì´ ë¹„ìš©: ${result['total_cost']:.4f}")
    print(f"í‰ê·  ì§€ì—°: {result['avg_latency_ms']:.0f}ms")
    
    # ë…¸ë“œ í†µê³„
    print("\nğŸ“Š ë…¸ë“œ í†µê³„:")
    stats = system.get_node_stats()
    print(f"ì‹œìŠ¤í…œ í™œìš©ë„: {stats['system_utilization']*100:.1f}%")
