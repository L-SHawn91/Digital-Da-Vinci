"""
ìºì‹± ì‹œìŠ¤í…œ - ì„±ëŠ¥ ìµœì í™”

ì—­í• :
- Multi-level ìºì‹± (Memory, Redis, Pinecone)
- API ì‘ë‹µ ìºì‹±
- ì‹ ê²½ê³„ ì‹ í˜¸ ìºì‹±
- ì¹´íŠ¸ë¦¬ì§€ ê²°ê³¼ ìºì‹±
"""

from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
import json
import hashlib
from functools import wraps
import time


class CacheManager:
    """ë©€í‹°ë ˆë²¨ ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.memory_cache = {}  # L1: ë©”ëª¨ë¦¬ ìºì‹œ
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'total_size': 0
        }
        self.max_memory_size = 100 * 1024 * 1024  # 100MB
        
    def _generate_key(self, *args, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_str = json.dumps({
            'args': str(args),
            'kwargs': str(sorted(kwargs.items()))
        })
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, key: str, ttl: int = 3600) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        if key in self.memory_cache:
            cached = self.memory_cache[key]
            
            # TTL í™•ì¸
            if datetime.now() - cached['timestamp'] < timedelta(seconds=ttl):
                self.cache_stats['hits'] += 1
                return cached['value']
            else:
                # TTL ë§Œë£Œ, ì‚­ì œ
                del self.memory_cache[key]
                self.cache_stats['evictions'] += 1
        
        self.cache_stats['misses'] += 1
        return None
    
    def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        """ìºì‹œì— ê°’ ì €ì¥"""
        size = len(json.dumps(value).encode())
        
        # ìš©ëŸ‰ ì´ˆê³¼ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì‚­ì œ
        if self.cache_stats['total_size'] + size > self.max_memory_size:
            self._evict_oldest()
        
        self.memory_cache[key] = {
            'value': value,
            'timestamp': datetime.now(),
            'ttl': ttl,
            'size': size
        }
        self.cache_stats['total_size'] += size
    
    def _evict_oldest(self) -> None:
        """ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°"""
        if not self.memory_cache:
            return
        
        oldest_key = min(
            self.memory_cache.items(),
            key=lambda x: x[1]['timestamp']
        )[0]
        
        removed_size = self.memory_cache[oldest_key]['size']
        del self.memory_cache[oldest_key]
        self.cache_stats['total_size'] -= removed_size
        self.cache_stats['evictions'] += 1
    
    def clear(self) -> None:
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.memory_cache.clear()
        self.cache_stats['total_size'] = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        total = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = (self.cache_stats['hits'] / total * 100) if total > 0 else 0
        
        return {
            'hits': self.cache_stats['hits'],
            'misses': self.cache_stats['misses'],
            'hit_rate': round(hit_rate, 2),
            'evictions': self.cache_stats['evictions'],
            'total_size_mb': round(self.cache_stats['total_size'] / 1024 / 1024, 2),
            'cached_items': len(self.memory_cache)
        }


class APIResponseCache:
    """API ì‘ë‹µ ìºì‹±"""
    
    def __init__(self, cache_manager: CacheManager):
        self.cache = cache_manager
    
    def cache_bio_analysis(self, image_path: str, ttl: int = 3600):
        """Bio ë¶„ì„ ê²°ê³¼ ìºì‹± ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                key = self.cache._generate_key('bio', image_path, *args, **kwargs)
                
                # ìºì‹œ í™•ì¸
                cached = self.cache.get(key, ttl)
                if cached is not None:
                    return cached
                
                # í•¨ìˆ˜ ì‹¤í–‰
                result = func(*args, **kwargs)
                
                # ê²°ê³¼ ìºì‹±
                self.cache.set(key, result, ttl)
                return result
            
            return wrapper
        return decorator
    
    def cache_stock_analysis(self, ticker: str, ttl: int = 900):
        """ì£¼ì‹ ë¶„ì„ ê²°ê³¼ ìºì‹± (15ë¶„)"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                key = self.cache._generate_key('stock', ticker, *args, **kwargs)
                
                cached = self.cache.get(key, ttl)
                if cached is not None:
                    return cached
                
                result = func(*args, **kwargs)
                self.cache.set(key, result, ttl)
                return result
            
            return wrapper
        return decorator


class NeuralSignalCache:
    """ì‹ ê²½ ì‹ í˜¸ ìºì‹±"""
    
    def __init__(self, cache_manager: CacheManager):
        self.cache = cache_manager
        self.signal_history = []
    
    def cache_neural_health(self, level: str, ttl: int = 60):
        """ì‹ ê²½ê³„ ê±´ê°•ë„ ìºì‹± (1ë¶„)"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                key = self.cache._generate_key('neural', level)
                
                cached = self.cache.get(key, ttl)
                if cached is not None:
                    return cached
                
                result = func(*args, **kwargs)
                self.cache.set(key, result, ttl)
                return result
            
            return wrapper
        return decorator
    
    def record_signal(self, level: str, health: float, latency: float) -> None:
        """ì‹ ê²½ ì‹ í˜¸ ê¸°ë¡"""
        signal = {
            'timestamp': datetime.now().isoformat(),
            'level': level,
            'health': health,
            'latency': latency
        }
        self.signal_history.append(signal)
        
        # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
        if len(self.signal_history) > 1000:
            self.signal_history = self.signal_history[-1000:]
    
    def get_signal_trend(self, level: str, minutes: int = 5) -> Dict[str, Any]:
        """ì‹ ê²½ ì‹ í˜¸ ì¶”ì„¸"""
        cutoff = datetime.now() - timedelta(minutes=minutes)
        
        recent_signals = [
            s for s in self.signal_history
            if s['level'] == level and 
            datetime.fromisoformat(s['timestamp']) > cutoff
        ]
        
        if not recent_signals:
            return {'status': 'no_data'}
        
        healths = [s['health'] for s in recent_signals]
        latencies = [s['latency'] for s in recent_signals]
        
        return {
            'level': level,
            'samples': len(recent_signals),
            'avg_health': sum(healths) / len(healths),
            'max_health': max(healths),
            'min_health': min(healths),
            'avg_latency': sum(latencies) / len(latencies),
            'trend': 'improving' if healths[-1] > healths[0] else 'degrading'
        }


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°"""
    
    def __init__(self):
        self.metrics = {
            'api_calls': [],
            'cartridge_calls': [],
            'neural_operations': []
        }
    
    def track_api_call(self, endpoint: str, duration: float, status: int):
        """API í˜¸ì¶œ ì¶”ì """
        self.metrics['api_calls'].append({
            'endpoint': endpoint,
            'duration': duration,
            'status': status,
            'timestamp': datetime.now().isoformat()
        })
    
    def track_cartridge_call(self, cartridge: str, operation: str, duration: float):
        """ì¹´íŠ¸ë¦¬ì§€ í˜¸ì¶œ ì¶”ì """
        self.metrics['cartridge_calls'].append({
            'cartridge': cartridge,
            'operation': operation,
            'duration': duration,
            'timestamp': datetime.now().isoformat()
        })
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸"""
        if not self.metrics['api_calls']:
            return {'status': 'no_data'}
        
        api_durations = [m['duration'] for m in self.metrics['api_calls']]
        
        return {
            'total_api_calls': len(self.metrics['api_calls']),
            'avg_api_latency': sum(api_durations) / len(api_durations),
            'max_api_latency': max(api_durations),
            'min_api_latency': min(api_durations),
            'p95_latency': sorted(api_durations)[int(len(api_durations) * 0.95)],
            'p99_latency': sorted(api_durations)[int(len(api_durations) * 0.99)]
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
cache_manager = CacheManager()
api_cache = APIResponseCache(cache_manager)
neural_cache = NeuralSignalCache(cache_manager)
perf_monitor = PerformanceMonitor()


if __name__ == "__main__":
    print("ğŸ’¾ ìºì‹± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥
    cache_manager.set('test_key', {'value': 'test_data'}, ttl=60)
    
    # ì¡°íšŒ
    result = cache_manager.get('test_key', ttl=60)
    print(f"ìºì‹œ ì¡°íšŒ: {result}")
    
    # í†µê³„
    print(f"ìºì‹œ í†µê³„: {cache_manager.get_stats()}")
