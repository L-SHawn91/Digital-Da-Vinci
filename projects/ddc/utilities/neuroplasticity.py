"""
ì‹ ê²½ê°€ì†Œì„± í•™ìŠµ ì‹œìŠ¤í…œ - ìë™ ìµœì í™”

ì—­í• :
- ì„±ëŠ¥ ë°ì´í„° ê¸°ë°˜ í•™ìŠµ
- ì‹ ê²½ ê°€ì¤‘ì¹˜ ìë™ ì¡°ì •
- ì ì‘í˜• ì„ê³„ê°’ ê´€ë¦¬
- ì¥ê¸° ë©”ëª¨ë¦¬ ì €ì¥
"""

from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import statistics
import json


@dataclass
class NeuralWeight:
    """ì‹ ê²½ ê°€ì¤‘ì¹˜"""
    layer: str  # L1, L2, L3, L4
    model: str  # ì‚¬ìš© ëª¨ë¸
    base_weight: float  # ê¸°ë³¸ê°’ (0-1)
    current_weight: float  # í˜„ì¬ê°’
    adjustment_history: List[float]  # ì¡°ì • ì´ë ¥
    last_updated: datetime = None
    
    def __post_init__(self):
        if self.last_updated is None:
            self.last_updated = datetime.now()


class NeuroplasticityEngine:
    """ì‹ ê²½ê°€ì†Œì„± ì—”ì§„"""
    
    def __init__(self):
        # ì‹ ê²½ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.weights = {
            'L1_Brainstem': NeuralWeight('L1', 'groq', 0.15, 0.15, []),
            'L2_Limbic': NeuralWeight('L2', 'gemini_flash', 0.12, 0.12, []),
            'L3_Neocortex_Occipital': NeuralWeight('L3', 'gemini_2_5_pro', 0.15, 0.15, []),
            'L3_Neocortex_Temporal': NeuralWeight('L3', 'claude_opus', 0.15, 0.15, []),
            'L3_Neocortex_Parietal': NeuralWeight('L3', 'deepseek', 0.12, 0.12, []),
            'L3_Neocortex_Prefrontal': NeuralWeight('L3', 'claude_opus', 0.18, 0.18, []),
            'L4_NeuroNet': NeuralWeight('L4', 'gemini_2_5_pro', 0.13, 0.13, [])
        }
        
        # ì ì‘í˜• ì„ê³„ê°’
        self.adaptive_thresholds = {
            'api_latency_ms': 100,
            'error_rate_percent': 5,
            'cache_hit_rate_percent': 50,
            'neural_health': 0.7
        }
        
        self.performance_history = []
        self.learning_rate = 0.01  # í•™ìŠµë¥ 
        self.min_weight = 0.05
        self.max_weight = 0.30
    
    def learn_from_performance(
        self,
        metrics: Dict[str, float],
        outcomes: Dict[str, bool]
    ) -> Dict[str, float]:
        """ì„±ëŠ¥ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµ"""
        adjustments = {}
        
        # ê° ì‹ ê²½ ê³„ì¸µë³„ í•™ìŠµ
        for weight_key, weight in self.weights.items():
            # ì„±ê³µë¥  ê³„ì‚°
            successes = sum(outcomes.values())
            total = len(outcomes)
            success_rate = successes / total if total > 0 else 0
            
            # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (0-1)
            api_score = max(0, 1 - (metrics.get('api_latency_ms', 100) / 200))
            cache_score = metrics.get('cache_hit_rate_percent', 50) / 100
            success_score = success_rate
            
            # ì¢…í•© ì ìˆ˜
            performance_score = (api_score + cache_score + success_score) / 3
            
            # ê°€ì¤‘ì¹˜ ì¡°ì •
            adjustment = (performance_score - 0.5) * self.learning_rate
            new_weight = max(self.min_weight, min(self.max_weight, weight.current_weight + adjustment))
            
            # íˆìŠ¤í† ë¦¬ ê¸°ë¡
            weight.adjustment_history.append(adjustment)
            if len(weight.adjustment_history) > 100:
                weight.adjustment_history = weight.adjustment_history[-100:]
            
            weight.current_weight = new_weight
            weight.last_updated = datetime.now()
            
            adjustments[weight_key] = {
                'previous': weight.current_weight - adjustment,
                'new': new_weight,
                'adjustment': adjustment,
                'performance_score': performance_score
            }
        
        self.performance_history.append({
            'timestamp': datetime.now().isoformat(),
            'metrics': metrics,
            'adjustments': adjustments
        })
        
        return adjustments
    
    def get_optimized_weights(self) -> Dict[str, float]:
        """ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì¡°íšŒ"""
        return {
            key: weight.current_weight
            for key, weight in self.weights.items()
        }
    
    def adapt_thresholds(self, recent_metrics: List[Dict[str, float]]):
        """ì„ê³„ê°’ ìë™ ì¡°ì •"""
        if len(recent_metrics) < 10:
            return
        
        # ìµœê·¼ ë©”íŠ¸ë¦­ ë¶„ì„
        api_latencies = [m.get('api_latency_ms', 100) for m in recent_metrics]
        error_rates = [m.get('error_rate_percent', 5) for m in recent_metrics]
        cache_hits = [m.get('cache_hit_rate_percent', 50) for m in recent_metrics]
        neural_healths = [m.get('neural_health', 0.7) for m in recent_metrics]
        
        # ë™ì  ì„ê³„ê°’ ê³„ì‚° (P95)
        self.adaptive_thresholds['api_latency_ms'] = sorted(api_latencies)[int(len(api_latencies) * 0.95)]
        self.adaptive_thresholds['error_rate_percent'] = statistics.mean(error_rates) * 1.5
        self.adaptive_thresholds['cache_hit_rate_percent'] = statistics.mean(cache_hits) * 0.9
        self.adaptive_thresholds['neural_health'] = statistics.mean(neural_healths) * 0.95
    
    def predict_performance(self, lookback_hours: int = 24) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì˜ˆì¸¡"""
        recent_history = [
            h for h in self.performance_history
            if datetime.fromisoformat(h['timestamp']) > datetime.now() - timedelta(hours=lookback_hours)
        ]
        
        if len(recent_history) < 5:
            return {'status': 'insufficient_data'}
        
        # íŠ¸ë Œë“œ ë¶„ì„
        api_trend = []
        for h in recent_history:
            api_trend.append(h['metrics'].get('api_latency_ms', 100))
        
        # ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ ê³„ì‚°
        if len(api_trend) >= 2:
            slope = (api_trend[-1] - api_trend[0]) / len(api_trend)
            predicted_latency = api_trend[-1] + slope
        else:
            predicted_latency = api_trend[-1] if api_trend else 100
        
        return {
            'predicted_api_latency_ms': round(predicted_latency, 2),
            'trend': 'improving' if slope < 0 else 'degrading',
            'confidence': 'high' if len(api_trend) > 20 else 'medium'
        }


class LongTermMemory:
    """ì¥ê¸° ë©”ëª¨ë¦¬ ì €ì¥"""
    
    def __init__(self):
        self.memory_entries = []
    
    def save_learning(self, learning_data: Dict[str, Any]) -> str:
        """í•™ìŠµ ê²°ê³¼ ì €ì¥"""
        entry = {
            'id': f"mem_{len(self.memory_entries)}_{datetime.now().timestamp()}",
            'timestamp': datetime.now().isoformat(),
            'data': learning_data,
            'importance': self._calculate_importance(learning_data)
        }
        
        self.memory_entries.append(entry)
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 100ê°œë§Œ ìœ ì§€
        self.memory_entries.sort(key=lambda x: x['importance'], reverse=True)
        if len(self.memory_entries) > 100:
            self.memory_entries = self.memory_entries[:100]
        
        return entry['id']
    
    def _calculate_importance(self, data: Dict[str, Any]) -> float:
        """ì¤‘ìš”ë„ ê³„ì‚°"""
        importance = 0
        
        if 'performance_improvement' in data:
            importance += data['performance_improvement'] * 10
        
        if 'anomaly_detected' in data and data['anomaly_detected']:
            importance += 5
        
        if 'model_change' in data:
            importance += 3
        
        return min(importance, 10)  # ìµœëŒ€ 10
    
    def get_important_memories(self, count: int = 10) -> List[Dict[str, Any]]:
        """ì¤‘ìš”í•œ ë©”ëª¨ë¦¬ ì¡°íšŒ"""
        return self.memory_entries[:count]
    
    def export_to_json(self) -> str:
        """JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        return json.dumps(self.memory_entries, indent=2)


class AdaptiveSystem:
    """ì ì‘í˜• ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.neuroplasticity = NeuroplasticityEngine()
        self.memory = LongTermMemory()
        self.last_adaptation = datetime.now()
        self.adaptation_interval = timedelta(minutes=5)
    
    def run_learning_cycle(
        self,
        metrics: Dict[str, float],
        outcomes: Dict[str, bool],
        recent_metrics: List[Dict[str, float]]
    ) -> Dict[str, Any]:
        """í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰"""
        
        # 1. ì‹ ê²½ê°€ì†Œì„± í•™ìŠµ
        adjustments = self.neuroplasticity.learn_from_performance(metrics, outcomes)
        
        # 2. ì„ê³„ê°’ ì ì‘
        self.neuroplasticity.adapt_thresholds(recent_metrics)
        
        # 3. ì„±ëŠ¥ ì˜ˆì¸¡
        prediction = self.neuroplasticity.predict_performance()
        
        # 4. í•™ìŠµ ê²°ê³¼ ì €ì¥
        learning_data = {
            'adjustments': adjustments,
            'thresholds': self.neuroplasticity.adaptive_thresholds,
            'prediction': prediction,
            'timestamp': datetime.now().isoformat()
        }
        
        memory_id = self.memory.save_learning(learning_data)
        
        # ë§ˆì§€ë§‰ ì ì‘ ì‹œê°„ ì—…ë°ì´íŠ¸
        self.last_adaptation = datetime.now()
        
        return {
            'status': 'success',
            'adjustments': adjustments,
            'prediction': prediction,
            'memory_id': memory_id,
            'weights': self.neuroplasticity.get_optimized_weights()
        }
    
    def should_adapt(self) -> bool:
        """ì ì‘ í•„ìš” ì—¬ë¶€"""
        return datetime.now() - self.last_adaptation >= self.adaptation_interval
    
    def get_system_report(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë¦¬í¬íŠ¸"""
        return {
            'timestamp': datetime.now().isoformat(),
            'weights': self.neuroplasticity.get_optimized_weights(),
            'thresholds': self.neuroplasticity.adaptive_thresholds,
            'memory_count': len(self.memory.memory_entries),
            'important_memories': self.memory.get_important_memories(5)
        }


if __name__ == "__main__":
    print("ğŸ§  ì‹ ê²½ê°€ì†Œì„± í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n")
    
    system = AdaptiveSystem()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    metrics = {
        'api_latency_ms': 55,
        'error_rate_percent': 0.5,
        'cache_hit_rate_percent': 87,
        'neural_health': 0.96
    }
    
    outcomes = {
        'bio_analysis': True,
        'stock_analysis': True,
        'text_analysis': True,
        'decision': True
    }
    
    recent_metrics = [
        {'api_latency_ms': 50 + i*2, 'cache_hit_rate_percent': 85 + i}
        for i in range(20)
    ]
    
    # í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰
    result = system.run_learning_cycle(metrics, outcomes, recent_metrics)
    
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"ì ì‘ëœ ê°€ì¤‘ì¹˜: {system.neuroplasticity.get_optimized_weights()}")
    print(f"ë©”ëª¨ë¦¬ ì €ì¥: {result['memory_id']}")
