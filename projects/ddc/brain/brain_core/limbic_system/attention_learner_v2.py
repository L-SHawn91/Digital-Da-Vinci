#!/usr/bin/env python3
"""
attention_learner.py - L2 ë³€ì—°ê³„: ìš°ì„ ìˆœìœ„ & í•™ìŠµ ì‹œìŠ¤í…œ (Week 5-6, Step 3)

ê°ì •ì— ë”°ë¥¸ ì¤‘ìš”ë„ íŒë‹¨ & ê°ì • ê¸°ë°˜ Q-Learning
ìƒí™©ë³„ ìµœì  ì „ëµ í•™ìŠµ + ì‹ ê²½ ì‹ í˜¸ ë¼ìš°íŒ…
"""

import json
import random
import statistics
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
import hashlib


@dataclass
class PriorityLevel:
    """ìš°ì„ ìˆœìœ„ ë ˆë²¨"""
    score: float  # 0-1
    level: str  # critical, high, medium, low
    reasoning: str
    action_required: bool


class PriorityCalculator:
    """ìš°ì„ ìˆœìœ„ ê³„ì‚° ì—”ì§„"""
    
    # ê°ì •ë³„ ê¸°ë³¸ ìš°ì„ ìˆœìœ„
    EMOTION_PRIORITY = {
        'happy': 0.3,      # ë‚®ìŒ (ìœ ì§€ë§Œ í•˜ë©´ ë¨)
        'sad': 0.8,        # ë†’ìŒ (ì§€ì› í•„ìš”)
        'angry': 0.9,      # ë§¤ìš° ë†’ìŒ (ì¦‰ì‹œ ëŒ€ì‘)
        'fear': 0.85,      # ë†’ìŒ (ì§€ì› í•„ìš”)
        'surprise': 0.4,   # ì¤‘ê°„ (ëŒ€ì‘ í•„ìš”)
        'neutral': 0.2,    # ë‚®ìŒ (ì •ë³´ ì „ë‹¬ë§Œ)
    }
    
    # ê°•ë„ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„ ë°°ìˆ˜
    INTENSITY_MULTIPLIER = {
        'weak': 0.5,       # <0.3
        'moderate': 1.0,   # 0.3-0.6
        'strong': 1.5,     # 0.6-0.8
        'intense': 2.0,    # >=0.8
    }
    
    def __init__(self):
        """ìš°ì„ ìˆœìœ„ ê³„ì‚°ê¸° ì´ˆê¸°í™”"""
        self.priority_history = []
        self.user_patterns = defaultdict(dict)
    
    def calculate(self, emotion: str, intensity: float, context: Dict = None) -> PriorityLevel:
        """
        ìš°ì„ ìˆœìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            emotion: ê°ì •
            intensity: ê°•ë„ (0-1)
            context: ë§¥ë½ ì •ë³´
            
        Returns:
            PriorityLevel: ìš°ì„ ìˆœìœ„ ë ˆë²¨
        """
        # 1. ê°ì • ê¸°ë³¸ ìš°ì„ ìˆœìœ„
        base_priority = self.EMOTION_PRIORITY.get(emotion, 0.5)
        
        # 2. ê°•ë„ ë°°ìˆ˜ ì ìš©
        intensity_level = self._get_intensity_level(intensity)
        multiplier = self.INTENSITY_MULTIPLIER.get(intensity_level, 1.0)
        
        # 3. ë§¥ë½ ìš”ì†Œ ì ìš©
        context_boost = 0.0
        reasoning = []
        
        if context:
            # ê¸´ê¸‰ ì—¬ë¶€ í™•ì¸
            if context.get('is_urgent', False):
                context_boost += 0.2
                reasoning.append('ê¸´ê¸‰ ìƒí™©')
            
            # ì§ˆë¬¸ì´ ë§ìœ¼ë©´ ì‘ë‹µ í•„ìš”
            if context.get('question_count', 0) > 1:
                context_boost += 0.15
                reasoning.append('ì—¬ëŸ¬ ì§ˆë¬¸ ê°ì§€')
            
            # ë©”ì‹œì§€ ê¸¸ì´ (ê¸¸ë©´ ì¤‘ìš”í•  ê°€ëŠ¥ì„±)
            if context.get('text_length', 0) > 100:
                context_boost += 0.1
                reasoning.append('ê¸´ ë©”ì‹œì§€')
        
        # 4. ìµœì¢… ìš°ì„ ìˆœìœ„ ê³„ì‚°
        final_priority = min(1.0, base_priority * multiplier + context_boost)
        
        # 5. ìš°ì„ ìˆœìœ„ ë ˆë²¨ ê²°ì •
        if final_priority >= 0.8:
            level = 'critical'
            action_required = True
        elif final_priority >= 0.6:
            level = 'high'
            action_required = True
        elif final_priority >= 0.4:
            level = 'medium'
            action_required = True
        else:
            level = 'low'
            action_required = False
        
        reasoning_text = ' + '.join(reasoning) if reasoning else 'ê°ì • ê¸°ë°˜ íŒë‹¨'
        
        result = PriorityLevel(
            score=round(final_priority, 2),
            level=level,
            reasoning=reasoning_text,
            action_required=action_required
        )
        
        self.priority_history.append({
            'emotion': emotion,
            'priority': result.score,
            'level': level,
            'timestamp': datetime.now().isoformat(),
        })
        
        return result
    
    def _get_intensity_level(self, intensity: float) -> str:
        """ê°•ë„ ë ˆë²¨ ê²°ì •"""
        if intensity < 0.3:
            return 'weak'
        elif intensity < 0.6:
            return 'moderate'
        elif intensity < 0.8:
            return 'strong'
        else:
            return 'intense'


class EmotionalQLearner:
    """ê°ì • ê¸°ë°˜ Q-Learning ì—”ì§„"""
    
    def __init__(self, learning_rate: float = 0.15, discount_factor: float = 0.85, exploration_rate: float = 0.20):
        """
        ê°ì • Q-Learning ì´ˆê¸°í™”
        
        Args:
            learning_rate: í•™ìŠµë¥  (Î±) - 0.15ë¡œ ë¹ ë¥¸ í•™ìŠµ
            discount_factor: í• ì¸ìœ¨ (Î³) - 0.85ë¡œ í˜„ì¬ ë§Œì¡±ë„ ì¤‘ì‹œ
            exploration_rate: íƒí—˜ë¥  (Îµ) - 0.20ìœ¼ë¡œ 20% íƒí—˜
        """
        self.alpha = learning_rate  # 0.15
        self.gamma = discount_factor  # 0.85
        self.epsilon = exploration_rate  # 0.20
        
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.emotion_weights = {
            'happy': 1.2,      # í–‰ë³µì€ ì•½ê°„ ê°€ì¤‘ì¹˜
            'sad': 1.8,        # ìŠ¬í””ì€ ë†’ì€ ê°€ì¤‘ì¹˜
            'angry': 2.0,      # ë¶„ë…¸ëŠ” ìµœê³  ê°€ì¤‘ì¹˜
            'fear': 1.8,       # ë‘ë ¤ì›€ì€ ë†’ì€ ê°€ì¤‘ì¹˜
            'surprise': 1.0,   # ë†€ëŒì€ ê¸°ë³¸
            'neutral': 0.8,    # ì¤‘ë¦½ì€ ë‚®ì€ ê°€ì¤‘ì¹˜
        }
        
        self.learning_history = []
        self.convergence_history = []
    
    def select_action(self, state: str, available_actions: List[str]) -> str:
        """
        Îµ-ê·¸ë¦¬ë”” ì •ì±…ìœ¼ë¡œ í–‰ë™ì„ ì„ íƒí•©ë‹ˆë‹¤.
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
            available_actions: ê°€ëŠ¥í•œ í–‰ë™ ëª©ë¡
            
        Returns:
            str: ì„ íƒëœ í–‰ë™
        """
        # 20% íƒí—˜, 80% í™œìš©
        if random.random() < self.epsilon:
            # íƒí—˜: ë¬´ì‘ìœ„ ì„ íƒ
            return random.choice(available_actions)
        else:
            # í™œìš©: ìµœê³  Qê°’ ì„ íƒ
            best_actions = []
            best_q = -float('inf')
            
            for action in available_actions:
                q_value = self.q_table[state][action]
                
                if q_value > best_q:
                    best_q = q_value
                    best_actions = [action]
                elif q_value == best_q:
                    best_actions.append(action)
            
            return random.choice(best_actions)
    
    def update_q_value(self, emotion: str, state: str, action: str, 
                      satisfaction: float, next_state: str, 
                      available_next_actions: List[str]) -> float:
        """
        ê°ì • ê°€ì¤‘ Q-Learningìœ¼ë¡œ Qê°’ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        ìˆ˜ì‹: Q(s,a) = Q(s,a) + Î±[rÂ·w_emotion + Î³Â·max(Q(s',a')) - Q(s,a)]
        
        Args:
            emotion: ê°ì •
            state: í˜„ì¬ ìƒíƒœ
            action: ì„ íƒëœ í–‰ë™
            satisfaction: ë§Œì¡±ë„ (0-10)
            next_state: ë‹¤ìŒ ìƒíƒœ
            available_next_actions: ë‹¤ìŒ ê°€ëŠ¥ í–‰ë™
            
        Returns:
            float: ì—…ë°ì´íŠ¸ëœ Qê°’
        """
        # í˜„ì¬ Qê°’
        current_q = self.q_table[state][action]
        
        # ê°ì • ê°€ì¤‘ì¹˜ ì ìš©
        emotion_weight = self.emotion_weights.get(emotion, 1.0)
        weighted_reward = satisfaction * emotion_weight
        
        # ë‹¤ìŒ ìƒíƒœì˜ ìµœê³  Qê°’
        max_future_q = 0.0
        if available_next_actions:
            max_future_q = max(self.q_table[next_state][a] for a in available_next_actions)
        
        # Bellman ë°©ì •ì‹
        new_q = current_q + self.alpha * (weighted_reward + self.gamma * max_future_q - current_q)
        
        # Q-table ì—…ë°ì´íŠ¸
        self.q_table[state][action] = new_q
        
        # í•™ìŠµ ì´ë ¥ ì €ì¥
        self.learning_history.append({
            'emotion': emotion,
            'state': state,
            'action': action,
            'satisfaction': satisfaction,
            'old_q': round(current_q, 2),
            'new_q': round(new_q, 2),
            'delta': round(new_q - current_q, 2),
            'timestamp': datetime.now().isoformat(),
        })
        
        return new_q
    
    def get_convergence_status(self) -> Dict:
        """
        í•™ìŠµ ìˆ˜ë ´ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Returns:
            Dict: ìˆ˜ë ´ ìƒíƒœ ì •ë³´
        """
        if len(self.learning_history) < 10:
            return {'status': 'insufficient_data', 'convergence_rate': 0.0}
        
        # ìµœê·¼ 100ê°œ í•™ìŠµ ì´ë ¥
        recent = self.learning_history[-100:]
        
        # Qê°’ ë³€í™”ëŸ‰ ë¶„ì„
        deltas = [abs(entry['delta']) for entry in recent]
        avg_delta = sum(deltas) / len(deltas)
        
        # í‘œì¤€í¸ì°¨ ê³„ì‚°
        variance = sum((d - avg_delta) ** 2 for d in deltas) / len(deltas)
        std_delta = variance ** 0.5
        
        # ìˆ˜ë ´ë„ ê³„ì‚° (ë³€í™”ê°€ ì ì„ìˆ˜ë¡ ìˆ˜ë ´)
        # 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìˆ˜ë ´, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë°œì‚°
        convergence_rate = min(1.0, avg_delta / (std_delta + 0.1)) if std_delta > 0 else 0.0
        
        status = 'converging' if convergence_rate < 0.3 else 'exploring'
        
        return {
            'status': status,
            'convergence_rate': round(convergence_rate, 2),
            'avg_delta': round(avg_delta, 2),
            'total_updates': len(self.learning_history),
        }
    
    def get_q_stats(self) -> Dict:
        """Q-table í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        if not self.q_table:
            return {}
        
        all_q_values = []
        for state_dict in self.q_table.values():
            all_q_values.extend(state_dict.values())
        
        if not all_q_values:
            return {}
        
        avg_q = sum(all_q_values) / len(all_q_values)
        variance = sum((q - avg_q) ** 2 for q in all_q_values) / len(all_q_values)
        std_q = variance ** 0.5
        
        return {
            'q_range': (round(min(all_q_values), 2), round(max(all_q_values), 2)),
            'avg_q': round(avg_q, 2),
            'std_q': round(std_q, 2),
            'total_states': len(self.q_table),
        }


class StrategyOptimizer:
    """ì „ëµ ìµœì í™” ì—”ì§„"""
    
    # ì „ëµ ì¹´í…Œê³ ë¦¬
    STRATEGIES = {
        'support': {
            'description': 'ê°ì •ì  ì§€ì›',
            'for_emotions': ['sad', 'fear'],
            'actions': ['empathize', 'listen', 'guide'],
        },
        'management': {
            'description': 'ê°ì • ê´€ë¦¬',
            'for_emotions': ['angry', 'fear'],
            'actions': ['calm', 'explain', 'redirect'],
        },
        'celebration': {
            'description': 'ì¶•í•˜/ê³µìœ ',
            'for_emotions': ['happy', 'surprise'],
            'actions': ['celebrate', 'encourage', 'share'],
        },
        'information': {
            'description': 'ì •ë³´ ì œê³µ',
            'for_emotions': ['neutral'],
            'actions': ['inform', 'explain', 'guide'],
        },
    }
    
    def __init__(self):
        """ì „ëµ ìµœì í™”ê¸° ì´ˆê¸°í™”"""
        self.strategy_performance = defaultdict(lambda: {'success': 0, 'total': 0})
        self.strategy_history = []
    
    def recommend_strategy(self, emotion: str, priority_level: str) -> Dict:
        """
        ê°ì •ê³¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ìµœì  ì „ëµì„ ì¶”ì²œí•©ë‹ˆë‹¤.
        
        Args:
            emotion: ê°ì •
            priority_level: ìš°ì„ ìˆœìœ„ ë ˆë²¨
            
        Returns:
            Dict: ì¶”ì²œ ì „ëµ
        """
        # ê°ì •ì— ë§ëŠ” ì „ëµ ì°¾ê¸°
        matching_strategies = []
        
        for strategy_name, strategy_info in self.STRATEGIES.items():
            if emotion in strategy_info['for_emotions']:
                matching_strategies.append((strategy_name, strategy_info))
        
        if not matching_strategies:
            matching_strategies = [('information', self.STRATEGIES['information'])]
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì „ëµ ì„ íƒ
        if priority_level == 'critical':
            # ê°€ì¥ íš¨ê³¼ì ì¸ ì „ëµ
            best_strategy = max(matching_strategies,
                              key=lambda x: self.strategy_performance[x[0]]['success'] / max(1, self.strategy_performance[x[0]]['total']))
        else:
            best_strategy = matching_strategies[0]
        
        return {
            'strategy': best_strategy[0],
            'description': best_strategy[1]['description'],
            'actions': best_strategy[1]['actions'],
            'emotion': emotion,
            'priority': priority_level,
        }
    
    def record_strategy_outcome(self, strategy: str, success: bool) -> None:
        """ì „ëµì˜ ì„±ê³µ ì—¬ë¶€ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
        self.strategy_performance[strategy]['total'] += 1
        if success:
            self.strategy_performance[strategy]['success'] += 1
        
        self.strategy_history.append({
            'strategy': strategy,
            'success': success,
            'timestamp': datetime.now().isoformat(),
        })
    
    def get_strategy_stats(self) -> Dict:
        """ì „ëµë³„ ì„±ê³µë¥ ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        stats = {}
        for strategy, perf in self.strategy_performance.items():
            if perf['total'] > 0:
                success_rate = perf['success'] / perf['total']
                stats[strategy] = {
                    'success_rate': round(success_rate, 2),
                    'total_uses': perf['total'],
                    'successes': perf['success'],
                }
        
        return stats


class NeuroSignalRouter:
    """ì‹ ê²½ ì‹ í˜¸ ë¼ìš°íŒ… (L4ë¡œ ì „ë‹¬)"""
    
    def __init__(self):
        """ì‹ ê²½ ì‹ í˜¸ ë¼ìš°í„° ì´ˆê¸°í™”"""
        self.routing_table = []
        self.signal_queue = []
    
    def route_signal(self, emotion: str, priority: float, strategy: str, 
                    satisfaction: float) -> Dict:
        """
        ì‹ ê²½ ì‹ í˜¸ë¥¼ ë‹¤ìŒ ê³„ì¸µ(L3/L4)ìœ¼ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
        
        Args:
            emotion: ê°ì •
            priority: ìš°ì„ ìˆœìœ„
            strategy: ì „ëµ
            satisfaction: ë§Œì¡±ë„
            
        Returns:
            Dict: ë¼ìš°íŒ…ëœ ì‹ í˜¸
        """
        signal = {
            'timestamp': datetime.now().isoformat(),
            'emotion': emotion,
            'priority': priority,
            'strategy': strategy,
            'satisfaction': satisfaction,
            'destination': self._determine_destination(priority, strategy),
            'urgency': 'immediate' if priority >= 0.8 else 'normal',
        }
        
        self.signal_queue.append(signal)
        return signal
    
    def _determine_destination(self, priority: float, strategy: str) -> str:
        """ë¼ìš°íŒ… ëª©ì ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
        if priority >= 0.9 and strategy in ['management', 'support']:
            return 'L3_executive_center'  # ê¸´ê¸‰: ì „ë‘ì—½ìœ¼ë¡œ
        elif priority >= 0.7:
            return 'L3_limbic_processing'  # ë†’ìŒ: ì¸¡ë‘ì—½ìœ¼ë¡œ
        else:
            return 'L3_standard_processing'  # ë‚®ìŒ: ê¸°ë³¸ ì²˜ë¦¬


class AttentionLearnerSystem:
    """í†µí•© ìš°ì„ ìˆœìœ„ & í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.priority_calc = PriorityCalculator()
        self.q_learner = EmotionalQLearner()
        self.strategy_opt = StrategyOptimizer()
        self.signal_router = NeuroSignalRouter()
    
    def process_emotion(self, emotion: str, intensity: float, 
                       satisfaction: float, context: Dict = None, 
                       user_id: str = None) -> Dict:
        """
        ê°ì •ì„ ì²˜ë¦¬í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤.
        
        Args:
            emotion: ê°ì •
            intensity: ê°•ë„
            satisfaction: ë§Œì¡±ë„ (0-10)
            context: ë§¥ë½
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼
        """
        # 1. ìš°ì„ ìˆœìœ„ ê³„ì‚°
        priority = self.priority_calc.calculate(emotion, intensity, context)
        
        # 2. ì „ëµ ì¶”ì²œ
        strategy_rec = self.strategy_opt.recommend_strategy(emotion, priority.level)
        
        # 3. Q-Learning ì—…ë°ì´íŠ¸
        state = f"{emotion}_{priority.level}"
        next_state = f"{emotion}_processed"
        action = strategy_rec['strategy']
        
        q_value = self.q_learner.update_q_value(
            emotion=emotion,
            state=state,
            action=action,
            satisfaction=satisfaction,
            next_state=next_state,
            available_next_actions=['support', 'management', 'celebration']
        )
        
        # 4. ì‹ ê²½ ì‹ í˜¸ ë¼ìš°íŒ…
        signal = self.signal_router.route_signal(
            emotion=emotion,
            priority=priority.score,
            strategy=action,
            satisfaction=satisfaction
        )
        
        # 5. ê²°ê³¼ ì¢…í•©
        result = {
            'emotion': emotion,
            'priority': {
                'score': priority.score,
                'level': priority.level,
                'reasoning': priority.reasoning,
                'action_required': priority.action_required,
            },
            'strategy': strategy_rec,
            'q_learning': {
                'state': state,
                'action': action,
                'q_value': round(q_value, 2),
            },
            'signal': signal,
            'convergence': self.q_learner.get_convergence_status(),
        }
        
        return result
    
    def get_system_report(self) -> Dict:
        """ì‹œìŠ¤í…œ ì „ì²´ ë¦¬í¬íŠ¸"""
        return {
            'q_learning_stats': self.q_learner.get_q_stats(),
            'convergence': self.q_learner.get_convergence_status(),
            'strategy_performance': self.strategy_opt.get_strategy_stats(),
            'total_signals': len(self.signal_router.signal_queue),
        }


# ============================================================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================================================

if __name__ == '__main__':
    print('=' * 80)
    print('ğŸ§  L2 ë³€ì—°ê³„: ìš°ì„ ìˆœìœ„ & í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸')
    print('=' * 80)
    
    system = AttentionLearnerSystem()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        ('happy', 0.85, 8.0),
        ('sad', 0.80, 6.0),
        ('angry', 1.00, 5.0),
        ('fear', 0.65, 4.0),
        ('surprise', 0.70, 7.0),
        ('neutral', 0.50, 5.0),
    ]
    
    print('\nğŸ“Š ê°ì • ì²˜ë¦¬ & í•™ìŠµ:\n')
    
    for emotion, intensity, satisfaction in test_cases:
        context = {
            'is_urgent': intensity > 0.8,
            'question_count': int(intensity * 3),
            'text_length': int(intensity * 200),
        }
        
        result = system.process_emotion(emotion, intensity, satisfaction, context)
        
        print(f'ê°ì •: {result["emotion"]} (ê°•ë„: {intensity})')
        print(f'ìš°ì„ ìˆœìœ„: {result["priority"]["level"]} ({result["priority"]["score"]})')
        print(f'ì „ëµ: {result["strategy"]["strategy"]} - {result["strategy"]["description"]}')
        print(f'Qê°’: {result["q_learning"]["q_value"]}')
        print(f'ì‹ ê²½ì‹ í˜¸: {result["signal"]["destination"]} (ê¸´ê¸‰ë„: {result["signal"]["urgency"]})')
        print(f'ìˆ˜ë ´ë„: {result["convergence"]["convergence_rate"]}')
        print('-' * 80)
    
    # ì‹œìŠ¤í…œ ë¦¬í¬íŠ¸
    print('\nğŸ“ˆ ì‹œìŠ¤í…œ ì „ì²´ ë¦¬í¬íŠ¸:\n')
    report = system.get_system_report()
    print(f'Q-Learning í†µê³„: {report["q_learning_stats"]}')
    print(f'ìˆ˜ë ´ ìƒíƒœ: {report["convergence"]}')
    print(f'ì „ëµ ì„±ëŠ¥: {report["strategy_performance"]}')
    print(f'ì´ ì‹ ê²½ì‹ í˜¸: {report["total_signals"]}')
    
    print('\nâœ… ìš°ì„ ìˆœìœ„ & í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!')
    print('=' * 80)
