"""
ğŸ§  MoltBot Obsidian Memory Integration
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

File: ~/.openclaw/workspace/obsidian_memory.py

Purpose: Obsidian Vaultë¥¼ ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì†ŒìŠ¤ë¡œ í™œìš©

Architecture: ì˜µì…˜ 3
- Obsidianì˜ ëª¨ë“  .md íŒŒì¼ì„ ë©”ëª¨ë¦¬ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
- í´ë”ë³„ ê°€ì¤‘ì¹˜ ì ìš©
- ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”

Author: MoltBot
Date: 2026-01-30
"""

import os
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)


class ObsidianMemory:
    """Obsidian Vaultë¥¼ ë©”ëª¨ë¦¬ë¡œ ì‚¬ìš©í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, vault_path: str):
        """
        ì´ˆê¸°í™”
        
        Args:
            vault_path: Obsidian Vault ê²½ë¡œ
        """
        self.vault_path = Path(vault_path)
        
        # ì„¤ì •
        self.included_folders = [
            "10-Projects",
            "20-Areas", 
            "30-Concepts",
            "40-Sources",
            "50-Lab",
            "60-Writing"
        ]
        
        self.excluded_folders = {
            ".obsidian", ".smart-env", ".smtcmp_json_db",
            "99-System", "80-Assets", "90-Archive"
        }
        
        self.folder_weights = {
            "10-Projects": 1.5,
            "50-Lab": 1.3,
            "20-Areas": 1.0,
            "40-Sources": 0.9,
            "30-Concepts": 0.8,
            "60-Writing": 0.7
        }
        
        # ìºì‹œ
        self.cache_dir = Path.home() / ".openclaw/workspace/.cache/obsidian-index"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_ttl = 3600  # 1ì‹œê°„
        
        # ì¸ë±ìŠ¤
        self.file_index = {}
        self._load_cache()
        
        logger.info(f"âœ… ObsidianMemory initialized: {vault_path}")
    
    def index_vault(self) -> Dict[str, Any]:
        """
        Obsidian Vault ì „ì²´ ì¸ë±ì‹±
        
        Returns:
            {
                "indexed_count": 125,
                "folders": ["10-Projects", ...],
                "file_count": {
                    "10-Projects": 45,
                    ...
                },
                "timestamp": "2026-01-30T22:10:00"
            }
        """
        
        indexed = 0
        folder_counts = {}
        all_files = []
        
        try:
            # ê° í´ë” ìˆœíšŒ
            for folder in self.included_folders:
                folder_path = self.vault_path / folder
                
                if not folder_path.exists():
                    logger.warning(f"âš ï¸ Folder not found: {folder}")
                    continue
                
                md_files = list(folder_path.rglob("*.md"))
                folder_counts[folder] = len(md_files)
                
                for file_path in md_files:
                    try:
                        # íŒŒì¼ ë©”íƒ€ë°ì´í„°
                        stat = file_path.stat()
                        
                        file_record = {
                            "path": str(file_path),
                            "relative_path": str(file_path.relative_to(self.vault_path)),
                            "folder": folder,
                            "name": file_path.stem,
                            "size": stat.st_size,
                            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                            "weight": self.folder_weights.get(folder, 1.0)
                        }
                        
                        all_files.append(file_record)
                        indexed += 1
                    
                    except Exception as e:
                        logger.error(f"âŒ Error processing {file_path}: {e}")
                        continue
            
            # ì¸ë±ìŠ¤ ì €ì¥
            self.file_index = {f["path"]: f for f in all_files}
            self._save_cache(all_files)
            
            result = {
                "indexed_count": indexed,
                "folders": list(folder_counts.keys()),
                "file_count": folder_counts,
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"âœ… Indexed {indexed} files from {len(folder_counts)} folders")
            return result
        
        except Exception as e:
            logger.error(f"âŒ Indexing failed: {e}")
            raise
    
    def search(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """
        ë©”ëª¨ë¦¬ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_results: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            [
                {
                    "path": "/10-Projects/...",
                    "folder": "10-Projects",
                    "name": "íŒŒì¼ëª…",
                    "weight": 1.5,
                    "modified": "2026-01-30T...",
                    "preview": "íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"
                },
                ...
            ]
        """
        
        if not self.file_index:
            self.index_vault()
        
        query_lower = query.lower()
        results = []
        
        try:
            for file_path, metadata in self.file_index.items():
                # 1. íŒŒì¼ëª…ìœ¼ë¡œ ë§¤ì¹­
                score = 0
                if query_lower in metadata["name"].lower():
                    score += 10 * metadata["weight"]
                
                # 2. í´ë”ëª…ìœ¼ë¡œ ë§¤ì¹­
                if query_lower in metadata["folder"].lower():
                    score += 5 * metadata["weight"]
                
                # 3. íŒŒì¼ ë‚´ìš© ê²€ìƒ‰ (ì„ íƒ)
                try:
                    content = Path(file_path).read_text(encoding='utf-8')
                    if query_lower in content.lower():
                        occurrences = content.lower().count(query_lower)
                        score += (2 + occurrences) * metadata["weight"]
                        
                        # ë¯¸ë¦¬ë³´ê¸° ì¶”ì¶œ
                        lines = content.split('\n')
                        preview = '\n'.join(lines[:3])[:200]
                        metadata["preview"] = preview
                
                except Exception as e:
                    logger.warning(f"âš ï¸ Cannot read {file_path}: {e}")
                
                if score > 0:
                    results.append({
                        **metadata,
                        "score": score
                    })
            
            # ì ìˆ˜ë¡œ ì •ë ¬
            results.sort(key=lambda x: x["score"], reverse=True)
            results = results[:max_results]
            
            logger.info(f"âœ… Found {len(results)} results for '{query}'")
            return results
        
        except Exception as e:
            logger.error(f"âŒ Search failed: {e}")
            return []
    
    def read_file(self, file_path: str) -> Optional[str]:
        """
        íŒŒì¼ ë‚´ìš© ì½ê¸°
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
        
        Returns:
            íŒŒì¼ ë‚´ìš© (Markdown)
        """
        
        try:
            full_path = Path(file_path)
            if not full_path.exists():
                full_path = self.vault_path / file_path
            
            content = full_path.read_text(encoding='utf-8')
            logger.info(f"âœ… Read {file_path} ({len(content)} bytes)")
            return content
        
        except Exception as e:
            logger.error(f"âŒ Cannot read {file_path}: {e}")
            return None
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        ë©”ëª¨ë¦¬ í†µê³„
        
        Returns:
            {
                "total_files": 125,
                "total_size_mb": 15.3,
                "folders": {...},
                "last_indexed": "2026-01-30T..."
            }
        """
        
        if not self.file_index:
            return {"status": "not_indexed"}
        
        total_size = sum(f["size"] for f in self.file_index.values())
        folder_stats = {}
        
        for folder in self.included_folders:
            folder_files = [f for f in self.file_index.values() if f["folder"] == folder]
            if folder_files:
                folder_stats[folder] = {
                    "file_count": len(folder_files),
                    "size_mb": sum(f["size"] for f in folder_files) / (1024*1024),
                    "weight": self.folder_weights.get(folder, 1.0)
                }
        
        return {
            "total_files": len(self.file_index),
            "total_size_mb": round(total_size / (1024*1024), 2),
            "folders": folder_stats,
            "vault_path": str(self.vault_path),
            "last_indexed": self._get_cache_time()
        }
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _save_cache(self, files: List[Dict]) -> None:
        """ìºì‹œ ì €ì¥"""
        cache_file = self.cache_dir / "vault-index.json"
        try:
            with open(cache_file, 'w') as f:
                json.dump(files, f, indent=2)
            logger.info(f"âœ… Cache saved: {cache_file}")
        except Exception as e:
            logger.warning(f"âš ï¸ Cache save failed: {e}")
    
    def _load_cache(self) -> bool:
        """ìºì‹œ ë¡œë“œ"""
        cache_file = self.cache_dir / "vault-index.json"
        
        if not cache_file.exists():
            return False
        
        # ìºì‹œ ìœ íš¨ ê¸°ê°„ í™•ì¸
        mtime = cache_file.stat().st_mtime
        if (datetime.now() - datetime.fromtimestamp(mtime)).total_seconds() > self.cache_ttl:
            logger.info("âš ï¸ Cache expired")
            return False
        
        try:
            with open(cache_file, 'r') as f:
                files = json.load(f)
            self.file_index = {f["path"]: f for f in files}
            logger.info(f"âœ… Cache loaded: {len(self.file_index)} files")
            return True
        except Exception as e:
            logger.warning(f"âš ï¸ Cache load failed: {e}")
            return False
    
    def _get_cache_time(self) -> Optional[str]:
        """ìºì‹œ ìƒì„± ì‹œê°„"""
        cache_file = self.cache_dir / "vault-index.json"
        if cache_file.exists():
            mtime = cache_file.stat().st_mtime
            return datetime.fromtimestamp(mtime).isoformat()
        return None


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì‚¬ìš© ì˜ˆì‹œ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # 1. ì´ˆê¸°í™”
    obsidian = ObsidianMemory(
        "/Users/soohyunglee/Library/CloudStorage/OneDrive-ê°œì¸/Obsidian/SHawn"
    )
    
    # 2. Vault ì¸ë±ì‹±
    print("ğŸ“‡ Indexing Obsidian Vault...")
    index_result = obsidian.index_vault()
    print(f"âœ… Indexed {index_result['indexed_count']} files")
    
    # 3. ê²€ìƒ‰
    print("\nğŸ” Searching...")
    results = obsidian.search("SHawn-Brain", max_results=5)
    for r in results:
        print(f"  â€¢ {r['relative_path']} (weight: {r['weight']})")
    
    # 4. í†µê³„
    print("\nğŸ“Š Statistics:")
    stats = obsidian.get_statistics()
    print(json.dumps(stats, indent=2, ensure_ascii=False))
