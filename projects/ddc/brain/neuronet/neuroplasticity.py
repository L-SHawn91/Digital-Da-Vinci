
"""
ì‹ ê²½ê°€ì†Œì„± (Neuroplasticity) v2.0 - Multi-Criteria Learning Engine
ë°•ì‚¬ë‹˜ 5ê³„ì¸µ í‰ê°€ í”„ë ˆì„ì›Œí¬ í†µí•© (íš¨ìœ¨ì„± ê³„ì¸µ ì¤‘ì‹¬)

[í•µì‹¬ ê°œì„ ]
1. Speed Score + Quality Score ë¶„ë¦¬
2. Layerë³„ ê°€ì¤‘ì¹˜ ì°¨ë“± (L1: ì†ë„80%, L3: í’ˆì§ˆ70%)
3. í† í° íš¨ìœ¨ì„± ì¶”ì 
"""

import os
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional

logger = logging.getLogger(__name__)

class NeuroplasticityLearner:
    """
    Multi-Criteria Adaptive Learning Engine
    ë°•ì‚¬ë‹˜ 5ê³„ì¸µ í”„ë ˆì„ì›Œí¬ ê¸°ë°˜ ê°•í™”í•™ìŠµ
    """
    
    # [v3.0] Layerë³„ í‰ê°€ ê°€ì¤‘ì¹˜ (ë°•ì‚¬ë‹˜ 5ê³„ì¸µ í”„ë ˆì„ì›Œí¬ ì™„ì „ ë°˜ì˜)
    LAYER_WEIGHTS = {
        "L1": {  # Reflexive: ì†ë„ + ì‹ ë¢°ì„± ìµœìš°ì„ 
            "speed": 0.45,
            "reliability": 0.30,  # [NEW] ì„±ê³µë¥  Ã— ì¼ê´€ì„±
            "cost": 0.15,
            "token_eff": 0.10
        },
        "L2": {  # Affective: ê· í˜• + ì¼ê´€ì„±
            "speed": 0.25,
            "quality": 0.40,
            "reliability": 0.20,
            "memory": 0.15
        },
        "L3": {  # Cognitive: í’ˆì§ˆ + íš¨ìœ¨ì„±
            "quality": 0.40,
            "token_eff": 0.25,
            "memory": 0.20,
            "speed": 0.15
        },
        "L4": {  # NeuroNet: ì°½ì˜ì„± + ì‹ ë¢°ì„±
            "quality": 0.35,
            "token_eff": 0.25,
            "reliability": 0.20,
            "cost": 0.10,
            "speed": 0.10
        }
    }

    # [v3.0] ì—”ì§„ë³„ ì´ˆê¸° ê°€ìƒ ì ìˆ˜ (6ì°¨ì› Prior)
    ENGINE_PRIORS = {
        "Groq": {
            "speed": 0.99, "quality": 0.60, "token_eff": 0.90, 
            "cost": 0.95, "memory": 0.50, "reliability": 0.95
        },
        "Gemini": {
            "speed": 0.50, "quality": 0.95, "token_eff": 0.80, 
            "cost": 0.85, "memory": 0.70, "reliability": 0.85
        },
        "Claude": {
            "speed": 0.40, "quality": 0.99, "token_eff": 0.85, 
            "cost": 0.30, "memory": 0.60, "reliability": 0.70  # í¬ë ˆë”§ ì´ìŠˆ
        },
        "DeepSeek": {
            "speed": 0.80, "quality": 0.70, "token_eff": 0.92, 
            "cost": 0.98, "memory": 0.65, "reliability": 0.85
        },
        "OpenAI": {
            "speed": 0.55, "quality": 0.90, "token_eff": 0.75, 
            "cost": 0.60, "memory": 0.70, "reliability": 0.80
        }
    }

    def __init__(self, memory_path=None):
        self.memory_path = memory_path or os.path.expanduser("~/.openclaw/workspace/neuro_memory.json")
        self.model_scores = {}
        self.learning_history = []
        self.learning_rate = 0.05
        
        self.load_weights()
    
    def load_weights(self):
        """ì €ì¥ëœ í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.memory_path):
            try:
                with open(self.memory_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # v2.0 ì²´í¬
                    if data.get("version") == "2.0":
                        self.model_scores = data.get("model_scores", {})
                    else:
                        # v1 -> v2 ë³€í™˜ ì‹œì—ë„ Prior ì ìš©
                        logger.info("ğŸ”„ Converting v1 weights with Engine Priors...")
                        # v1 ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸°í™”
                        self.model_scores = self._convert_v1_to_v2(data.get("weights", {}))
                    
                    self.learning_history = data.get("history", [])
                    logger.info(f"ğŸ§  Loaded neuro-memory: {len(self.model_scores)} models tracked")
            except Exception as e:
                logger.error(f"Failed to load neuro-memory: {e}")
        else:
            logger.info("ğŸ§  No previous memory found. Starting fresh with Priors.")

    def _convert_v1_to_v2(self, old_weights: dict) -> dict:
        """v1 (ë‹¨ì¼ ê°€ì¤‘ì¹˜) -> v3 (6ì°¨ì› ì§€í‘œ) ë³€í™˜"""
        new_scores = {}
        for model_id, weight in old_weights.items():
            new_scores[model_id] = {
                "speed": 0.5,
                "quality": weight,
                "token_eff": 0.5,
                "cost": 0.5,
                "memory": 0.5,
                "reliability": 0.8,  # ì´ˆê¸° ì‹ ë¢°ë„
                "success_count": 0,
                "call_count": 0,
                "latency_history": []  # ì†ë„ ë³€ë™ì„± ì¶”ì 
            }
        return new_scores

    def save_weights(self):
        """í•™ìŠµ ë°ì´í„° ì˜êµ¬ ì €ì¥ (v2 í˜•ì‹)"""
        try:
            os.makedirs(os.path.dirname(self.memory_path), exist_ok=True)
            data = {
                "version": "2.0",
                "model_scores": self.model_scores,
                "history": self.learning_history[-100:],
                "last_updated": datetime.now().isoformat()
            }
            with open(self.memory_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logger.debug("ğŸ’¾ Neuro-weights saved (v2).")
        except Exception as e:
            logger.error(f"Failed to save neuro-memory: {e}")

    def record_interaction(
        self, 
        user_id: str, 
        model_id: str, 
        context: dict, 
        latency_ms: float,
        quality_score: float = 0.8,
        tokens_used: int = 0,
        memory_latency: float = 0.0,
        is_success: bool = True  # [NEW] ì„±ê³µ/ì‹¤íŒ¨ ëª…ì‹œ
    ):
        """
        ìƒí˜¸ì‘ìš© ê¸°ë¡ ë° í•™ìŠµ (6-Criteria Hebbian Update + Reliability)
        """
        level = context.get("level", "L3")
        
        # 1. Speed Score ê³„ì‚°
        target_latency = {"L1": 1000, "L2": 3000, "L3": 10000, "L4": 5000}.get(level, 10000)
        speed_score = max(0.1, min(1.0, target_latency / max(latency_ms, 1)))
        
        # 2. Token Efficiency (í† í° íš¨ìœ¨ì„±: ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
        # 1000í† í° ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™” (0~1000 í† í° -> 1.0~0.0)
        token_eff_score = max(0.1, min(1.0, 1.0 - (tokens_used / 2000.0)))
        
        # 3. Cost (ë¹„ìš© íš¨ìœ¨ì„±: ëª¨ë¸ë³„ ë‹¨ê°€ ê³ ë ¤)
        # ê°„ë‹¨í•œ ì¶”ì •: Groq/DeepSeek = ì €ë ´(0.9), Gemini = ì¤‘ê°„(0.7), Claude/OpenAI = ë¹„ìŒˆ(0.3)
        cost_score = 0.7  # ê¸°ë³¸ê°’ (ì¶”í›„ ì‹¤ì œ API ë‹¨ê°€ ë°˜ì˜)
        
        # 4. Memory Overhead (ë©”ëª¨ë¦¬ ë¶€ë‹´ë„: ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        total_time = latency_ms + memory_latency
        memory_score = 1.0 - (memory_latency / max(total_time, 1)) if total_time > 0 else 1.0
        
        # 5. Model Scores ì—…ë°ì´íŠ¸ (v3.1: 6ì°¨ì› + Reliability)
        if model_id not in self.model_scores:
            self.model_scores[model_id] = {
                "speed": 0.5,
                "quality": 0.5,
                "token_eff": 0.5,
                "cost": 0.5,
                "memory": 0.5,
                "reliability": 0.8,
                "success_count": 0,
                "call_count": 0,
                "latency_history": []
            }
        
        current = self.model_scores[model_id]
        
        # Success Rate ì—…ë°ì´íŠ¸
        if is_success:
            current["success_count"] += 1
        current["call_count"] += 1
        success_rate = current["success_count"] / max(current["call_count"], 1)
        
        # Latency Variance (ì¼ê´€ì„±) ê³„ì‚°
        current["latency_history"].append(latency_ms)
        if len(current["latency_history"]) > 20:  # ìµœê·¼ 20ê°œë§Œ ìœ ì§€
            current["latency_history"] = current["latency_history"][-20:]
        
        import statistics
        if len(current["latency_history"]) > 2:
            stdev = statistics.stdev(current["latency_history"])
            mean = statistics.mean(current["latency_history"])
            # CV (Coefficient of Variation): ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì 
            cv = stdev / max(mean, 1)
            consistency = max(0.1, 1.0 - min(cv, 1.0))  # ë³€ë™ í´ìˆ˜ë¡ ë‚®ìŒ
        else:
            consistency = 0.8  # ì´ˆê¸°ê°’
        
        # Reliability = Success Rate Ã— Consistency
        reliability_score = success_rate * consistency
        
        # í•™ìŠµ ë°˜ì˜
        current["speed"] += self.learning_rate * (speed_score - current["speed"])
        current["quality"] += self.learning_rate * (quality_score - current["quality"])
        current["token_eff"] += self.learning_rate * (token_eff_score - current["token_eff"])
        current["cost"] += self.learning_rate * (cost_score - current["cost"])
        current["memory"] += self.learning_rate * (memory_score - current["memory"])
        current["reliability"] += self.learning_rate * (reliability_score - current["reliability"])
        
        # 3. History Logging
        record = {
            "timestamp": datetime.now().isoformat(),
            "user": user_id,
            "model": model_id,
            "level": level,
            "latency_ms": latency_ms,
            "speed_score": speed_score,
            "quality_score": quality_score,
            "tokens": tokens_used,
            "memory_ms": memory_latency
        }
        self.learning_history.append(record)
        
        # 4. Auto Save
        self.save_weights()
        logger.info(f"ğŸ§  Learning: {model_id} | Speed={speed_score:.2f} Quality={quality_score:.2f}")

    def select_best_model(self, level: str, candidates: List[dict]) -> str:
        """
        Layerë³„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ìµœì  ëª¨ë¸ ì„ íƒ (v3.1: 6ì°¨ì› ì§€í‘œ)
        """
        if not candidates:
            return "gemini-2.0-flash"
        
        # 1. Exploration (10%)
        if len(candidates) > 1 and (hash(datetime.now()) % 100) < 10:
            picked = candidates[hash(datetime.now()) % len(candidates)]
            return picked["id"]
        
        # 2. Exploitation (6ì°¨ì› ê°€ì¤‘ í•©ì‚°)
        weights = self.LAYER_WEIGHTS.get(level, {"speed": 0.5, "quality": 0.5})
        best_model = candidates[0]
        best_score = -1.0
        
        for model in candidates:
            model_id = model["id"]
            engine = model.get("engine", "Unknown")
            
            # ëª¨ë¸ ì ìˆ˜ ì¡°íšŒ (ì—†ìœ¼ë©´ ë°•ì‚¬ë‹˜ Priors ì ìš©)
            if model_id not in self.model_scores:
                prior = self.ENGINE_PRIORS.get(engine, {
                    "speed": 0.5, "quality": 0.5, "token_eff": 0.5, 
                    "cost": 0.5, "memory": 0.5, "reliability": 0.8
                })
                scores = prior
            else:
                scores = self.model_scores[model_id]
            
            # ê°€ì¤‘ í•©ì‚° (ì§€í‘œë³„ë¡œ ê°€ì¤‘ì¹˜ ìˆìœ¼ë©´ ì ìš©, ì—†ìœ¼ë©´ Skip)
            final_score = 0.0
            for metric, weight in weights.items():
                final_score += scores.get(metric, 0.5) * weight
            
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            noise = (hash(model_id + str(datetime.now())) % 100) / 10000.0
            final_score += noise
            
            if final_score > best_score:
                best_score = final_score
                best_model = model
                
        logger.info(f"ğŸ§  [{level}] Select: {best_model['id']} | Score={best_score:.3f}")
        return best_model["id"]
    
    def rank_models(self, level: str, candidates: List[dict]) -> List[tuple[dict, float]]:
        """
        ì‹ ê²½ê°€ì†Œì„± í•™ìŠµ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  í›„ë³´ë¥¼ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        
        Returns:
            List[(model_dict, score)]: ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ëœ (ëª¨ë¸, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates:
            return []
        
        weights = self.LAYER_WEIGHTS.get(level, {"speed": 0.5, "quality": 0.5})
        ranked = []
        
        for model in candidates:
            model_id = model["id"]
            engine = model.get("engine", "Unknown")
            
            # ëª¨ë¸ ì ìˆ˜ ì¡°íšŒ (ì—†ìœ¼ë©´ Priors ì ìš©)
            if model_id not in self.model_scores:
                prior = self.ENGINE_PRIORS.get(engine, {
                    "speed": 0.5, "quality": 0.5, "token_eff": 0.5, 
                    "cost": 0.5, "memory": 0.5, "reliability": 0.8
                })
                scores = prior
            else:
                scores = self.model_scores[model_id]
            
            # ê°€ì¤‘ í•©ì‚°
            final_score = 0.0
            for metric, weight in weights.items():
                final_score += scores.get(metric, 0.5) * weight
            
            ranked.append((model, final_score))
        
        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        ranked.sort(key=lambda x: x[1], reverse=True)
        
        logger.info(f"ğŸ§  [{level}] Ranked {len(ranked)} models | Top: {ranked[0][0]['id']} ({ranked[0][1]:.3f})")
        return ranked
