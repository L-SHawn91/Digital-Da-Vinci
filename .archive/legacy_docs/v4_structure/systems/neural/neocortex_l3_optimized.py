"""
L3 ì‹ í”¼ì§ˆ ìµœì í™” ëª¨ë“ˆ - ê° ì—½ ì„±ëŠ¥ ê°•í™”

ëª©í‘œ: 8.5/10 ë‹¬ì„±

ê°œì„  ì‚¬í•­:
1. ì „ë‘ì—½: ë” ì •êµí•œ ê³„íš ì•Œê³ ë¦¬ì¦˜
2. ì¸¡ë‘ì—½: ê¸°ì–µ ê¸°ë°˜ ë§¥ë½ ì¶”ë¡ 
3. ë‘ì •ì—½: í–¥ìƒëœ í†µí•© ë¡œì§
4. í›„ë‘ì—½: ì‹¬ì¸µ ë°ì´í„° ë¶„ì„
"""

import json
from datetime import datetime
from typing import Dict, List, Tuple
from enum import Enum
import logging

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)


class OptimizedPrefrontalCortex:
    """ìµœì í™”ëœ ì „ë‘ì—½: ê³ ê¸‰ ê³„íš & ì˜ì‚¬ê²°ì •"""
    
    def __init__(self):
        # ì „ëµë³„ ìš°ì„ ìˆœìœ„ ë§¤íŠ¸ë¦­ìŠ¤
        self.strategy_matrix = {
            "research": {
                "critical": ["ë…¼ë¬¸ ê²€ìˆ˜", "ì‹ ê²½ê³„ ë¶„ì„", "ëª¨ë¸ ê²€ì¦"],
                "high": ["ë°ì´í„° ì •ë¦¬", "ì‹¤í—˜ ì„¤ê³„", "ê²°ê³¼ ë¶„ì„"],
                "medium": ["ë¬¸í—Œ ê²€í† ", "ì•„ì´ë””ì–´ ì •ë¦¬"]
            },
            "development": {
                "critical": ["ë²„ê·¸ ìˆ˜ì •", "ì„±ëŠ¥ ìµœì í™”"],
                "high": ["ê¸°ëŠ¥ ì¶”ê°€", "ì½”ë“œ ë¦¬ë·°"],
                "medium": ["ë¬¸ì„œí™”", "í…ŒìŠ¤íŠ¸"]
            },
            "general": {
                "critical": ["ê¸´ê¸‰ ëŒ€ì‘"],
                "high": ["ì£¼ìš” ì‘ì—…"],
                "medium": ["ì¼ë°˜ ì‘ì—…"]
            }
        }

    def plan_action_advanced(self, emotion: str, priority: str, text: str) -> Dict:
        """ê³ ê¸‰ í–‰ë™ ê³„íš"""
        
        # 1. ë„ë©”ì¸ ë¶„ë¥˜
        domain = self._classify_domain(text)
        
        # 2. ê°ì • ê°€ì¤‘ì¹˜ ì ìš©
        emotion_weight = self._get_emotion_weight(emotion, priority)
        
        # 3. ê³„íš ìƒì„±
        plans = self._generate_tiered_plans(domain, priority)
        
        # 4. ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(domain, emotion, priority)
        
        return {
            "domain": domain,
            "primary_action": plans[0] if plans else "ì²˜ë¦¬",
            "backup_actions": plans[1:] if len(plans) > 1 else [],
            "emotion_factor": emotion_weight,
            "confidence": confidence
        }

    def _classify_domain(self, text: str) -> str:
        """ë„ë©”ì¸ ë¶„ë¥˜"""
        text_lower = text.lower()
        
        if any(kw in text_lower for kw in ["ë…¼ë¬¸", "ì—°êµ¬", "ë¶„ì„", "ì‹ ê²½"]):
            return "research"
        elif any(kw in text_lower for kw in ["ì½”ë“œ", "ë²„ê·¸", "ê°œë°œ", "ìµœì í™”"]):
            return "development"
        else:
            return "general"

    def _get_emotion_weight(self, emotion: str, priority: str) -> float:
        """ê°ì • ê°€ì¤‘ì¹˜"""
        emotion_map = {
            "joy": 1.1,      # ê¸ì •ì  â†’ íš¨ìœ¨ì„± ì¦ê°€
            "anger": 0.9,    # ë¶€ì •ì  â†’ ì‹ ì¤‘í•¨
            "anxiety": 0.8,  # ë¶ˆì•ˆ â†’ ì¡°ì‹¬ìŠ¤ëŸ¬ì›€
            "sadness": 0.85, # ìŠ¬í”” â†’ ì‹ ì¤‘í•¨
            "neutral": 1.0,  # ì¤‘ë¦½ â†’ ê¸°ë³¸
            "mixed": 0.95    # í˜¼í•© â†’ ê· í˜•
        }
        
        base_weight = emotion_map.get(emotion, 1.0)
        
        # ìš°ì„ ìˆœìœ„ê°€ ë†’ì„ìˆ˜ë¡ ê°ì • ì˜í–¥ ê°ì†Œ
        if priority == "critical":
            return base_weight * 0.8
        
        return base_weight

    def _generate_tiered_plans(self, domain: str, priority: str) -> List[str]:
        """ê³„ì¸µí™”ëœ ê³„íš ìƒì„±"""
        plans = self.strategy_matrix.get(domain, {}).get(priority, [])
        return plans[:3]  # ìƒìœ„ 3ê°œ

    def _calculate_confidence(self, domain: str, emotion: str, priority: str) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        domain_confidence = {"research": 0.90, "development": 0.88, "general": 0.80}
        emotion_confidence = {"neutral": 0.95, "joy": 0.92, "mixed": 0.85}
        priority_confidence = {"critical": 0.92, "high": 0.88, "medium": 0.85, "low": 0.80}
        
        avg = (
            domain_confidence.get(domain, 0.85) +
            emotion_confidence.get(emotion, 0.80) +
            priority_confidence.get(priority, 0.85)
        ) / 3
        
        return round(avg, 3)


class OptimizedTemporalCortex:
    """ìµœì í™”ëœ ì¸¡ë‘ì—½: ê¸°ì–µ & ë§¥ë½ ê°•í™”"""
    
    def __init__(self):
        self.episodic_memory = []  # ì‚¬ê±´ ê¸°ì–µ
        self.semantic_memory = {}  # ì˜ë¯¸ ê¸°ì–µ
        self.max_memory_size = 100

    def analyze_context_advanced(self, text: str, emotion: str, priority: str) -> Dict:
        """ê³ ê¸‰ ë§¥ë½ ë¶„ì„"""
        
        # 1. ì˜ë¯¸ ê¸°ì–µ ê²€ìƒ‰
        semantic_context = self._search_semantic_memory(text)
        
        # 2. íŒ¨í„´ ì¸ì‹ (ê³ ê¸‰)
        patterns = self._recognize_advanced_patterns(text, emotion, priority)
        
        # 3. ë§¥ë½ ì ìˆ˜
        context_score = self._calculate_context_score(semantic_context, patterns)
        
        # 4. ì—°ê´€ ì‚¬ê±´ ì°¾ê¸°
        related_events = self._find_related_events(text)
        
        return {
            "semantic_context": semantic_context,
            "patterns": patterns,
            "context_score": context_score,
            "related_events": related_events,
            "memory_confidence": min(len(self.episodic_memory) / 50, 1.0)
        }

    def _search_semantic_memory(self, text: str) -> str:
        """ì˜ë¯¸ ê¸°ì–µ ê²€ìƒ‰"""
        text_lower = text.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ë¯¸ ë§¤í•‘
        if any(kw in text_lower for kw in ["ë…¼ë¬¸", "ê²€ìˆ˜", "í”¼ë“œë°±"]):
            return "academic_research"
        elif any(kw in text_lower for kw in ["ì‹ ê²½ê³„", "ëª¨ë¸", "ìµœì í™”"]):
            return "technical_work"
        elif any(kw in text_lower for kw in ["í•©ê²©", "ê²°ê³¼", "ì„±ê³µ"]):
            return "achievement"
        else:
            return "general_matter"

    def _recognize_advanced_patterns(self, text: str, emotion: str, priority: str) -> List[str]:
        """ê³ ê¸‰ íŒ¨í„´ ì¸ì‹"""
        patterns = []
        
        # íŒ¨í„´ 1: ê°ì •-ìš°ì„ ìˆœìœ„ ì¡°í•©
        if emotion in ["anxiety", "mixed"] and priority in ["high", "critical"]:
            patterns.append("emotional_urgency")
        
        # íŒ¨í„´ 2: ë„ë©”ì¸ íŠ¹í™”
        if any(kw in text.lower() for kw in ["ë…¼ë¬¸", "ì‹ ê²½", "ëª¨ë¸"]):
            patterns.append("research_intensive")
        
        # íŒ¨í„´ 3: í•™ìŠµ ê¸°íšŒ
        if any(kw in text.lower() for kw in ["ìƒˆë¡œìš´", "ë°°ìš°", "ê°œì„ ", "ìµœì í™”"]):
            patterns.append("learning_opportunity")
        
        # íŒ¨í„´ 4: í”¼ë“œë°± ë£¨í”„
        if any(kw in text.lower() for kw in ["ê²€ìˆ˜", "í”¼ë“œë°±", "ê²€í† ", "í‰ê°€"]):
            patterns.append("feedback_loop")
        
        return patterns

    def _calculate_context_score(self, semantic_context: str, patterns: List[str]) -> float:
        """ë§¥ë½ ì ìˆ˜"""
        # ê¸°ë³¸ ì ìˆ˜
        base_score = 0.75
        
        # ì˜ë¯¸ ì ìˆ˜ ì¶”ê°€
        if semantic_context != "general_matter":
            base_score += 0.1
        
        # íŒ¨í„´ ì ìˆ˜ ì¶”ê°€
        base_score += len(patterns) * 0.05
        
        return min(round(base_score, 3), 1.0)

    def _find_related_events(self, text: str) -> List[str]:
        """ì—°ê´€ëœ ì‚¬ê±´ ì°¾ê¸°"""
        # ì‹œë®¬ë ˆì´ì…˜: ìµœê·¼ ê¸°ì–µì—ì„œ ì—°ê´€ ê²€ìƒ‰
        related = []
        
        if self.episodic_memory:
            # í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬
            for memory in self.episodic_memory[-5:]:
                if any(kw in memory.lower() for kw in text.lower().split()):
                    related.append(memory)
        
        return related[:3]

    def store_episodic_memory(self, text: str):
        """ì‚¬ê±´ ê¸°ì–µ ì €ì¥"""
        self.episodic_memory.append(text)
        
        if len(self.episodic_memory) > self.max_memory_size:
            self.episodic_memory.pop(0)


class OptimizedParietalCortex:
    """ìµœì í™”ëœ ë‘ì •ì—½: í†µí•© & ê³µê°„ ê°•í™”"""
    
    def integrate_lobes_advanced(self, prefrontal: Dict, temporal: Dict, 
                                occipital: Dict) -> Dict:
        """í–¥ìƒëœ ì—½ ê°„ í†µí•©"""
        
        # 1. ì •ë³´ ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = self._calculate_integration_weights(prefrontal, temporal, occipital)
        
        # 2. ì‹ ë¢°ë„ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        final_priority = self._resolve_priority_conflict(
            prefrontal, temporal, occipital, weights
        )
        
        # 3. í†µí•© ì‹ ë¢°ë„
        integration_confidence = self._calculate_integration_confidence(weights)
        
        # 4. ëŒ€ì•ˆ ê³„íš
        alternative_plans = self._generate_alternatives(prefrontal, temporal)
        
        return {
            "final_priority": final_priority,
            "integration_weights": weights,
            "integration_confidence": integration_confidence,
            "primary_action": prefrontal.get("primary_action"),
            "alternative_actions": alternative_plans,
            "rationale": f"Based on {final_priority} priority and {temporal.get('semantic_context')} context"
        }

    def _calculate_integration_weights(self, prefrontal: Dict, temporal: Dict, 
                                       occipital: Dict) -> Dict[str, float]:
        """í†µí•© ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        return {
            "prefrontal": prefrontal.get("confidence", 0.85),
            "temporal": temporal.get("context_score", 0.75),
            "occipital": occipital.get("analysis_depth", 0.80)
        }

    def _resolve_priority_conflict(self, prefrontal: Dict, temporal: Dict, 
                                   occipital: Dict, weights: Dict) -> str:
        """ìš°ì„ ìˆœìœ„ ì¶©ëŒ í•´ê²°"""
        # ê°€ì¤‘ í‰ê·  ê¸°ë°˜ ê²°ì •
        if temporal.get("patterns") and "emotional_urgency" in temporal["patterns"]:
            return "critical"
        elif temporal.get("semantic_context") == "academic_research":
            return "high"
        else:
            return "medium"

    def _calculate_integration_confidence(self, weights: Dict) -> float:
        """í†µí•© ì‹ ë¢°ë„"""
        values = list(weights.values())
        return round(sum(values) / len(values), 3)

    def _generate_alternatives(self, prefrontal: Dict, temporal: Dict) -> List[str]:
        """ëŒ€ì•ˆ ê³„íš ìƒì„±"""
        alternatives = []
        
        backup_actions = prefrontal.get("backup_actions", [])
        if backup_actions:
            alternatives.append(backup_actions[0])
        
        if temporal.get("semantic_context") == "academic_research":
            alternatives.append("ë¬¸í—Œ ì¬ê²€í† ")
        
        return alternatives[:2]


class OptimizedOccipitalCortex:
    """ìµœì í™”ëœ í›„ë‘ì—½: ì‹œê° & ë¶„ì„ ê°•í™”"""
    
    def analyze_data_advanced(self, text: str, emotion: str, priority: str) -> Dict:
        """ê³ ê¸‰ ë°ì´í„° ë¶„ì„"""
        
        # 1. ê¹Šì´ ìˆëŠ” í…ìŠ¤íŠ¸ ë¶„ì„
        text_analysis = self._deep_text_analysis(text)
        
        # 2. ê°ì •-í…ìŠ¤íŠ¸ ìƒê´€ê´€ê³„
        emotional_correlation = self._analyze_emotional_correlation(text, emotion)
        
        # 3. ìš°ì„ ìˆœìœ„ ì •ë‹¹ì„±
        priority_justification = self._justify_priority(text, priority)
        
        # 4. ì‹ í˜¸ ê°•ë„
        signal_strength = self._calculate_signal_strength(text, emotion, priority)
        
        return {
            "text_analysis": text_analysis,
            "emotional_correlation": emotional_correlation,
            "priority_justification": priority_justification,
            "signal_strength": signal_strength,
            "analysis_depth": 0.85
        }

    def _deep_text_analysis(self, text: str) -> Dict:
        """ê¹Šì´ ìˆëŠ” í…ìŠ¤íŠ¸ ë¶„ì„"""
        words = text.split()
        sentences = text.split(".")
        
        # í‚¤ì›Œë“œ ë°€ë„ ë¶„ì„
        keywords = ["ì‹ ê²½", "ëª¨ë¸", "ë…¼ë¬¸", "ë¶„ì„", "ìµœì í™”"]
        keyword_density = sum(1 for word in words if any(kw in word.lower() for kw in keywords)) / len(words) if words else 0
        
        return {
            "word_count": len(words),
            "sentence_count": len([s for s in sentences if s.strip()]),
            "avg_word_length": sum(len(w) for w in words) / len(words) if words else 0,
            "keyword_density": round(keyword_density, 3),
            "complexity_level": "high" if keyword_density > 0.2 else "medium" if len(words) > 15 else "low"
        }

    def _analyze_emotional_correlation(self, text: str, emotion: str) -> Dict:
        """ê°ì •-í…ìŠ¤íŠ¸ ìƒê´€ê´€ê³„"""
        text_lower = text.lower()
        
        emotional_markers = {
            "anxiety": ["ë¶ˆì•ˆ", "ê±±ì •", "ë‘ë ¤ì›€"],
            "joy": ["ê¸°ì¨", "ì¢‹ìŒ", "ì„¤ë ˜"],
            "urgency": ["ê¸´ê¸‰", "í•„ìš”", "ì¤‘ìš”"],
            "reflection": ["ìƒê°", "í‰ê°€", "ê²€í† "]
        }
        
        correlations = {}
        for marker_type, markers in emotional_markers.items():
            count = sum(1 for marker in markers if marker in text_lower)
            correlations[marker_type] = count
        
        return correlations

    def _justify_priority(self, text: str, priority: str) -> str:
        """ìš°ì„ ìˆœìœ„ ì •ë‹¹ì„±"""
        text_lower = text.lower()
        
        if priority == "critical":
            if "ì‹ ê²½ê³„" in text_lower and "ëª¨ë¸" in text_lower:
                return "Technical urgency: Neural system + Model optimization"
            elif "ë…¼ë¬¸ê²€ìˆ˜" in text_lower or "í”¼ë“œë°±" in text_lower:
                return "Academic urgency: Paper review feedback"
            else:
                return "Critical priority assigned"
        elif priority == "high":
            if "ë¶ˆì•ˆ" in text_lower or "ê±±ì •" in text_lower:
                return "Emotional urgency: Anxiety/Concern detected"
            else:
                return "High priority assigned"
        else:
            return "Standard priority processing"

    def _calculate_signal_strength(self, text: str, emotion: str, priority: str) -> float:
        """ì‹ í˜¸ ê°•ë„"""
        base_strength = 0.5
        
        # ê°ì • ê°•ë„
        emotion_strength = {"anxiety": 0.9, "joy": 0.8, "mixed": 0.85}
        base_strength += emotion_strength.get(emotion, 0.5) * 0.2
        
        # ìš°ì„ ìˆœìœ„ ê°•ë„
        priority_strength = {"critical": 0.9, "high": 0.7, "medium": 0.5, "low": 0.3}
        base_strength += priority_strength.get(priority, 0.5) * 0.3
        
        # í…ìŠ¤íŠ¸ ì‹ í˜¸
        if len(text) > 50:
            base_strength += 0.1
        
        return round(min(base_strength, 1.0), 3)


class OptimizedNeocortexL3:
    """ìµœì í™”ëœ L3 ì‹ í”¼ì§ˆ ì „ì²´"""
    
    def __init__(self):
        self.prefrontal = OptimizedPrefrontalCortex()
        self.temporal = OptimizedTemporalCortex()
        self.parietal = OptimizedParietalCortex()
        self.occipital = OptimizedOccipitalCortex()
        self.processing_log = []

    def process_optimized(self, text: str, emotion: str, priority: str) -> Dict:
        """ìµœì í™”ëœ ì²˜ë¦¬"""
        
        logger.info(f"ğŸ§  L3 ìµœì í™” ì²˜ë¦¬: {text[:30]}...")
        
        # ê° ì—½ ì²˜ë¦¬
        prefrontal_output = self.prefrontal.plan_action_advanced(emotion, priority, text)
        temporal_output = self.temporal.analyze_context_advanced(text, emotion, priority)
        occipital_output = self.occipital.analyze_data_advanced(text, emotion, priority)
        
        # í†µí•©
        parietal_output = self.parietal.integrate_lobes_advanced(
            prefrontal_output, temporal_output, occipital_output
        )
        
        # ìµœì¢… ê²°ê³¼
        result = {
            "timestamp": datetime.now().isoformat(),
            "input": text[:50],
            "prefrontal": prefrontal_output,
            "temporal": temporal_output,
            "parietal": parietal_output,
            "occipital": occipital_output,
            "final_action": parietal_output["primary_action"],
            "final_priority": parietal_output["final_priority"],
            "confidence": parietal_output["integration_confidence"],
            "rationale": parietal_output["rationale"]
        }
        
        # ê¸°ì–µ ì €ì¥
        self.temporal.store_episodic_memory(text)
        
        self.processing_log.append(result)
        
        return result


def main():
    """í…ŒìŠ¤íŠ¸"""
    logger.info("=" * 70)
    logger.info("ğŸ”§ L3 ì‹ í”¼ì§ˆ ìµœì í™” - ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 70)
    
    l3 = OptimizedNeocortexL3()
    
    test_cases = [
        ("ì‹ ê²½ê³„ ëª¨ë¸ ìµœì í™”ê°€ í•„ìš”í•´ìš”.", "neutral", "critical"),
        ("ê¸°ì˜ì§€ë§Œ ë™ì‹œì— ë¶ˆì•ˆí•´ìš”. í•©ê²©í–ˆëŠ”ë° ì¤€ë¹„ê°€ ë¶€ì¡±í•œ ê²ƒ ê°™ì•„ì„œ.", "mixed", "high"),
        ("ë…¼ë¬¸ ê²€ìˆ˜ í”¼ë“œë°±ì„ ë°›ì•˜ì–´ìš”. ì •ë§ ì¤‘ìš”í•©ë‹ˆë‹¤.", "anxiety", "high"),
        ("ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ì„¤ê³„í•´ì•¼ í•´ìš”.", "joy", "high"),
    ]
    
    results = []
    confidence_scores = []
    
    for text, emotion, priority in test_cases:
        logger.info(f"\nğŸ“ ì²˜ë¦¬: {text[:40]}...")
        
        result = l3.process_optimized(text, emotion, priority)
        results.append(result)
        confidence_scores.append(result["confidence"])
        
        logger.info(f"  âœ… í–‰ë™: {result['final_action']}")
        logger.info(f"  ğŸ¯ ìš°ì„ ìˆœìœ„: {result['final_priority']}")
        logger.info(f"  ğŸ“Š ì‹ ë¢°ë„: {result['confidence']:.3f}")
        logger.info(f"  ğŸ’¡ ê·¼ê±°: {result['rationale']}")
    
    avg_confidence = sum(confidence_scores) / len(confidence_scores)
    
    logger.info("\n" + "=" * 70)
    logger.info(f"âœ… L3 ìµœì í™” ì™„ë£Œ: {len(results)}ê°œ í…ŒìŠ¤íŠ¸")
    logger.info(f"ğŸ“Š í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
    logger.info(f"ğŸ¯ ëª©í‘œ: 8.5/10 (í˜„ì¬: {avg_confidence*10:.1f}/10)")
    logger.info("=" * 70)
    
    # JSON ì €ì¥
    with open("/Users/soohyunglee/.openclaw/workspace/systems/neural/l3_optimized_results.json", "w") as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "system": "L3 Optimized Neocortex",
            "total_tests": len(results),
            "average_confidence": avg_confidence,
            "score_out_of_10": avg_confidence * 10,
            "results": results
        }, f, indent=2, ensure_ascii=False)
    
    return results


if __name__ == "__main__":
    main()
