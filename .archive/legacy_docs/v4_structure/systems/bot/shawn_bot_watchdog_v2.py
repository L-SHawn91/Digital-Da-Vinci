"""
# shawn_bot_watchdog_v2.py - L1 ë‡Œê°„ ì‹ ê²½í•™ìŠµ ì‹œìŠ¤í…œ

ìˆ€ë´‡ L1 ë‡Œê°„ Watchdog (ì‹ ê²½ê³„ ê¸°ë°˜ ê°•í™”í•™ìŠµ)

ì—­í• :
  â”œâ”€ í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§ (5ì´ˆ ì£¼ê¸°)
  â”œâ”€ ìë™ ë³µêµ¬ (5ê°€ì§€ ì „ëµ)
  â”œâ”€ ê°•í™”í•™ìŠµ (Q-Learningìœ¼ë¡œ ìµœì  ì „ëµ í•™ìŠµ)
  â”œâ”€ í’ˆì§ˆ ì ìˆ˜ (ë³µêµ¬ìœ¨/íš¨ìœ¨ì„±/ì•ˆì •ì„±)
  â””â”€ ì¼ì¼ ë¦¬í¬íŠ¸ (ì„±ê³¼ ì¶”ì )

ì‹ ê²½ì „ë‹¬ë¬¼ì§ˆ: ì•„ë“œë ˆë‚ ë¦° (Adrenaline)
ëª©í‘œ: ì•ˆì •ì„± 3/10 â†’ 10/10 (99.99% ê°€ìš©ì„±)

Week 1-3 êµ¬í˜„ ë¡œë“œë§µ:
  Week 1 (ì§„í–‰ ì¤‘): ì‹ ê²½í•™ìŠµ ì‹œìŠ¤í…œ êµ¬í˜„ (~1,550ì¤„)
  Week 2: ì„±ëŠ¥ ìµœì í™” (ë³µêµ¬ì‹œê°„ -33%, ë³µêµ¬ìœ¨ +20%)
  Week 3: ì™„ì„± (ì•ˆì •ì„± 10/10, ë§ˆì¼ìŠ¤í†¤ 6.5/10)

State â†’ Action â†’ Reward â†’ Q-Learning â†’ ì ìˆ˜ ì—…ë°ì´íŠ¸
"""

import os
import sys
import time
import subprocess
import logging
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import signal
import psutil
import random
from dataclasses import dataclass, asdict
from enum import Enum
import hashlib


# ============================================================================
# 1. ìƒíƒœ(State) ì •ì˜
# ============================================================================

class ProcessStatus(Enum):
    """í”„ë¡œì„¸ìŠ¤ ìƒíƒœ"""
    RUNNING = "running"
    DOWN = "down"
    SLEEPING = "sleeping"
    ERROR = "error"
    ZOMBIE = "zombie"


class ProcessState:
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ìƒíƒœ"""
    
    def __init__(self, process_pid: Optional[int] = None):
        self.process_pid = process_pid
        self.status = ProcessStatus.DOWN
        self.memory_pct = 0.0
        self.cpu_pct = 0.0
        self.last_restart_time_ms = 0
        self.consecutive_restarts = 0
        self.last_successful_restart_time = datetime.now()
    
    def encode(self) -> str:
        """Stateë¥¼ í•´ì‹œë¡œ ë³€í™˜"""
        state_dict = {
            'status': self.status.value,
            'memory_pct': round(self.memory_pct, 1),
            'cpu_pct': round(self.cpu_pct, 1),
            'consecutive_restarts': self.consecutive_restarts
        }
        state_json = json.dumps(state_dict, sort_keys=True)
        return hashlib.md5(state_json.encode()).hexdigest()
    
    def __str__(self) -> str:
        return f"State(status={self.status.value}, mem={self.memory_pct:.1f}%, cpu={self.cpu_pct:.1f}%, restarts={self.consecutive_restarts})"


# ============================================================================
# 2. ì•¡ì…˜(Action) ì •ì˜
# ============================================================================

class ActionType(Enum):
    """ë³µêµ¬ ì „ëµ"""
    RESTART_IMMEDIATELY = "restart_immediately"
    CHECK_DEPENDENCIES_FIRST = "check_dependencies_first"
    WAIT_AND_RETRY = "wait_and_retry"
    ESCALATE_TO_MANUAL = "escalate_to_manual"
    RESTART_WITH_CLEAN_ENV = "restart_with_clean_env"


class Action:
    """í–‰ë™"""
    
    def __init__(self, action_type: ActionType):
        self.action_type = action_type
        self.times_taken = 0
        self.success_count = 0
        self.total_reward = 0.0
        self.avg_recovery_time_ms = 0.0
    
    def get_id(self) -> str:
        return self.action_type.value


# ============================================================================
# 3. ë³´ìƒ(Reward) & í’ˆì§ˆ ì ìˆ˜
# ============================================================================

class RewardCalculator:
    """ë³´ìƒ ê³„ì‚°"""
    
    # ë³´ìƒ ìƒìˆ˜
    RECOVERY_SUCCESS = 10.0
    RECOVERY_TIME_BONUS_3S = 5.0
    RECOVERY_TIME_3_5S = 3.0
    RECOVERY_TIME_PENALTY_5S = -2.0
    RECOVERY_FAILURE = -10.0
    CONSECUTIVE_FAILURE_PENALTY = -5.0
    
    @staticmethod
    def calculate_reward(
        success: bool,
        recovery_time_ms: int,
        consecutive_failures: int
    ) -> float:
        """
        ë³´ìƒ ê³„ì‚°
        
        Args:
            success: ë³µêµ¬ ì„±ê³µ ì—¬ë¶€
            recovery_time_ms: ì¬ì‹œì‘ ì†Œìš”ì‹œê°„ (ms)
            consecutive_failures: ì—°ì† ì‹¤íŒ¨ íšŸìˆ˜
        
        Returns:
            ë³´ìƒ ì ìˆ˜
        """
        if not success:
            reward = RewardCalculator.RECOVERY_FAILURE
            if consecutive_failures >= 2:
                reward -= RewardCalculator.CONSECUTIVE_FAILURE_PENALTY
            return reward
        
        # ì„±ê³µ ì‹œ
        reward = RewardCalculator.RECOVERY_SUCCESS
        
        # ì‹œê°„ ê¸°ë°˜ ë³´ë„ˆìŠ¤/í˜ë„í‹°
        if recovery_time_ms < 3000:
            reward += RewardCalculator.RECOVERY_TIME_BONUS_3S
        elif recovery_time_ms < 5000:
            reward += RewardCalculator.RECOVERY_TIME_3_5S
        else:
            reward += RewardCalculator.RECOVERY_TIME_PENALTY_5S
        
        return reward


class QualityScorer:
    """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (NeuralExecutor íŒ¨í„´)"""
    
    def __init__(self):
        self.success_count = 0
        self.total_attempts = 0
        self.total_recovery_time_ms = 0
        self.uptime_seconds = 0
        self.total_monitored_seconds = 0
    
    def calculate_recovery_rate(self) -> float:
        """
        ë³µêµ¬ìœ¨ (Weights: 40%)
        = success_count / total_attempts
        """
        if self.total_attempts == 0:
            return 0.0
        return min(1.0, self.success_count / self.total_attempts)
    
    def calculate_efficiency(self, target_time_ms: int = 5000) -> float:
        """
        íš¨ìœ¨ì„± (Weights: 30%)
        = (target_time - avg_time) / target_time
        ë²”ìœ„: 0.0 - 1.0
        """
        if self.total_attempts == 0:
            return 0.0
        
        avg_time_ms = self.total_recovery_time_ms / self.total_attempts
        efficiency = max(0.0, (target_time_ms - avg_time_ms) / target_time_ms)
        return min(1.0, efficiency)
    
    def calculate_stability(self, target_uptime_pct: float = 99.99) -> float:
        """
        ì•ˆì •ì„± (Weights: 30%)
        = uptime_pct / target_uptime_pct
        ë²”ìœ„: 0.0 - 1.0
        """
        if self.total_monitored_seconds == 0:
            return 0.0
        
        uptime_pct = (self.uptime_seconds / self.total_monitored_seconds) * 100
        stability = min(1.0, uptime_pct / target_uptime_pct)
        return stability
    
    def get_quality_score(self, target_time_ms: int = 5000) -> float:
        """
        ìµœì¢… í’ˆì§ˆ ì ìˆ˜ (0-100)
        = recovery_rate * 40 + efficiency * 30 + stability * 30
        """
        recovery_rate = self.calculate_recovery_rate()
        efficiency = self.calculate_efficiency(target_time_ms)
        stability = self.calculate_stability()
        
        score = (
            recovery_rate * 0.40 * 100 +
            efficiency * 0.30 * 100 +
            stability * 0.30 * 100
        )
        
        return round(score, 2)
    
    def record_attempt(self, success: bool, recovery_time_ms: int, uptime_ms: int):
        """ì‹œë„ ê¸°ë¡"""
        self.total_attempts += 1
        if success:
            self.success_count += 1
        self.total_recovery_time_ms += recovery_time_ms
        self.uptime_seconds += uptime_ms / 1000.0
        self.total_monitored_seconds += (recovery_time_ms / 1000.0) + (uptime_ms / 1000.0)


# ============================================================================
# 4. Q-Learning: ì‹ ê²½í•™ìŠµ (NeuralLearner)
# ============================================================================

class NeuralLearner:
    """Q-Learning ê¸°ë°˜ ì‹ ê²½í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self, learning_rate: float = 0.1, discount_factor: float = 0.9, epsilon: float = 0.15):
        self.learning_rate = learning_rate  # Î±
        self.discount_factor = discount_factor  # Î³
        self.epsilon = epsilon  # íƒí—˜ë¥ 
        
        # Q-Table: (state_hash, action_id) â†’ Q-value
        self.q_table: Dict[Tuple[str, str], float] = {}
        
        # í•™ìŠµ ê¸°ë¡
        self.learning_history: List[Dict] = []
        
        # í–‰ë™ë³„ í†µê³„
        self.action_stats: Dict[str, Dict] = {
            action.value: {
                'times_taken': 0,
                'success_count': 0,
                'total_recovery_time_ms': 0,
                'success_rate': 0.0
            }
            for action in ActionType
        }
    
    def get_q_value(self, state_hash: str, action_id: str) -> float:
        """Q-ê°’ ì¡°íšŒ"""
        key = (state_hash, action_id)
        return self.q_table.get(key, 0.0)
    
    def select_action(self, state_hash: str, available_actions: List[ActionType]) -> ActionType:
        """
        Îµ-ê·¸ë¦¬ë”” ì•¡ì…˜ ì„ íƒ
        - í™•ë¥  Îµ: ë¬´ì‘ìœ„ ì„ íƒ (íƒí—˜)
        - í™•ë¥  1-Îµ: ìµœê³  Q-ê°’ ì„ íƒ (í™œìš©)
        """
        if random.random() < self.epsilon:
            # íƒí—˜: ë¬´ì‘ìœ„ ì„ íƒ
            return random.choice(available_actions)
        else:
            # í™œìš©: ìµœê³  Q-ê°’ ì„ íƒ
            best_action = None
            best_q_value = float('-inf')
            
            for action in available_actions:
                action_id = action.value
                q_value = self.get_q_value(state_hash, action_id)
                
                if q_value > best_q_value:
                    best_q_value = q_value
                    best_action = action
            
            return best_action or random.choice(available_actions)
    
    def update_q_value(
        self,
        state_hash: str,
        action_id: str,
        reward: float,
        next_state_hash: str,
        next_actions: List[ActionType]
    ):
        """
        Q-value ì—…ë°ì´íŠ¸ (Bellman ë°©ì •ì‹)
        Q(s,a) = Q(s,a) + Î±[r + Î³Â·max(Q(s',a')) - Q(s,a)]
        """
        current_q = self.get_q_value(state_hash, action_id)
        
        # ë‹¤ìŒ ìƒíƒœì˜ ìµœê³  Q-ê°’
        max_next_q = 0.0
        if next_actions:
            max_next_q = max(
                self.get_q_value(next_state_hash, a.value)
                for a in next_actions
            )
        
        # ìƒˆë¡œìš´ Q-ê°’ ê³„ì‚°
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        # Q-Table ì—…ë°ì´íŠ¸
        key = (state_hash, action_id)
        self.q_table[key] = new_q
        
        # í•™ìŠµ ê¸°ë¡
        learning_log = {
            'timestamp': datetime.now().isoformat(),
            'state_hash': state_hash,
            'action_id': action_id,
            'reward': reward,
            'q_value': round(new_q, 4),
            'next_state_hash': next_state_hash,
            'max_next_q': round(max_next_q, 4)
        }
        self.learning_history.append(learning_log)
    
    def record_action_result(self, action_id: str, success: bool, recovery_time_ms: int):
        """í–‰ë™ ê²°ê³¼ ê¸°ë¡"""
        if action_id not in self.action_stats:
            return
        
        stats = self.action_stats[action_id]
        stats['times_taken'] += 1
        
        if success:
            stats['success_count'] += 1
        
        stats['total_recovery_time_ms'] += recovery_time_ms
        stats['success_rate'] = stats['success_count'] / stats['times_taken']
    
    def get_best_action(self, state_hash: str, available_actions: List[ActionType]) -> ActionType:
        """í˜„ì¬ ìƒíƒœì—ì„œ ìµœê³  Q-ê°’ì„ ê°€ì§„ ì•¡ì…˜"""
        best_action = available_actions[0]
        best_q_value = self.get_q_value(state_hash, best_action.value)
        
        for action in available_actions[1:]:
            q_value = self.get_q_value(state_hash, action.value)
            if q_value > best_q_value:
                best_q_value = q_value
                best_action = action
        
        return best_action
    
    def get_policy_report(self) -> Dict:
        """ì •ì±… ë¦¬í¬íŠ¸"""
        return {
            'timestamp': datetime.now().isoformat(),
            'q_table_size': len(self.q_table),
            'learning_steps': len(self.learning_history),
            'total_actions_taken': sum(s['times_taken'] for s in self.action_stats.values()),
            'action_statistics': self.action_stats,
            'recent_learning': self.learning_history[-10:] if self.learning_history else []
        }


# ============================================================================
# 5. í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘ (ProcessRestarter)
# ============================================================================

class ProcessRestarter:
    """í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘ ê´€ë¦¬"""
    
    def __init__(self, script_path: str, venv_path: str = ".venv_bot"):
        self.script_path = script_path
        self.venv_path = venv_path
        self.python_path = os.path.join(venv_path, "bin", "python3")
        self.logger = logging.getLogger("ProcessRestarter")
    
    def restart_immediately(self) -> Tuple[bool, int]:
        """ì¦‰ì‹œ ì¬ì‹œì‘"""
        try:
            start_time = time.time()
            process = subprocess.Popen(
                [self.python_path, self.script_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            recovery_time_ms = int((time.time() - start_time) * 1000)
            return True, recovery_time_ms
        except Exception as e:
            self.logger.error(f"ì¦‰ì‹œ ì¬ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False, 0
    
    def check_dependencies_first(self) -> Tuple[bool, int]:
        """ì˜ì¡´ì„± í™•ì¸ í›„ ì¬ì‹œì‘"""
        try:
            start_time = time.time()
            
            # ì˜ì¡´ì„± í™•ì¸
            subprocess.run(
                [self.python_path, "-m", "pip", "install", "-r", "requirements.txt"],
                capture_output=True,
                timeout=30
            )
            
            # ì¬ì‹œì‘
            process = subprocess.Popen(
                [self.python_path, self.script_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            recovery_time_ms = int((time.time() - start_time) * 1000)
            return True, recovery_time_ms
        except Exception as e:
            self.logger.error(f"ì˜ì¡´ì„± í™•ì¸ í›„ ì¬ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False, 0
    
    def wait_and_retry(self, wait_seconds: int = 2) -> Tuple[bool, int]:
        """ëŒ€ê¸° í›„ ì¬ì‹œë„"""
        try:
            start_time = time.time()
            
            # ëŒ€ê¸°
            time.sleep(wait_seconds)
            
            # ì¬ì‹œì‘
            process = subprocess.Popen(
                [self.python_path, self.script_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            recovery_time_ms = int((time.time() - start_time) * 1000)
            return True, recovery_time_ms
        except Exception as e:
            self.logger.error(f"ëŒ€ê¸° í›„ ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
            return False, 0
    
    def restart_with_clean_env(self) -> Tuple[bool, int]:
        """í™˜ê²½ ì´ˆê¸°í™” í›„ ì¬ì‹œì‘"""
        try:
            start_time = time.time()
            
            # í™˜ê²½ ì´ˆê¸°í™”
            env = os.environ.copy()
            env['PYTHONDONTWRITEBYTECODE'] = '1'
            env['PYTHONUNBUFFERED'] = '1'
            
            # í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘
            process = subprocess.Popen(
                [self.python_path, self.script_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env
            )
            
            recovery_time_ms = int((time.time() - start_time) * 1000)
            return True, recovery_time_ms
        except Exception as e:
            self.logger.error(f"í™˜ê²½ ì´ˆê¸°í™” í›„ ì¬ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False, 0


# ============================================================================
# 6. ë©”ì¸ Watchdog ì‹œìŠ¤í…œ (BotWatchdogV2)
# ============================================================================

class BotWatchdogV2:
    """L1 ë‡Œê°„ Watchdog - ì‹ ê²½ê³„ ê¸°ë°˜ ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        script_path: str,
        venv_path: str = ".venv_bot",
        monitor_interval: int = 5
    ):
        self.script_path = script_path
        self.venv_path = venv_path
        self.monitor_interval = monitor_interval
        
        # í•µì‹¬ ëª¨ë“ˆ
        self.neural_learner = NeuralLearner()
        self.quality_scorer = QualityScorer()
        self.process_restarter = ProcessRestarter(script_path, venv_path)
        
        # ìƒíƒœ
        self.process_state = ProcessState()
        self.process_pid = None
        self.process_alive = False
        self.consecutive_failures = 0
        
        # ë¡œê¹…
        self.logger = logging.getLogger("BotWatchdogV2")
        self._setup_logging()
        
        # í†µê³„
        self.start_time = datetime.now()
        self.daily_stats = []
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_dir = Path("logs/watchdog")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        log_file = log_dir / f"{datetime.now().strftime('%Y%m%d')}_watchdog_v2.log"
        
        handler = logging.FileHandler(log_file)
        handler.setLevel(logging.DEBUG)
        
        formatter = logging.Formatter(
            '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        )
        handler.setFormatter(formatter)
        
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.DEBUG)
    
    def _detect_state(self) -> ProcessState:
        """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ìƒíƒœ ê°ì§€"""
        state = ProcessState(self.process_pid)
        
        # í”„ë¡œì„¸ìŠ¤ í™œì„± ì—¬ë¶€ í™•ì¸
        if self.process_pid is None or not psutil.pid_exists(self.process_pid):
            state.status = ProcessStatus.DOWN
        else:
            try:
                proc = psutil.Process(self.process_pid)
                state.status = ProcessStatus.RUNNING
                state.memory_pct = proc.memory_percent()
                state.cpu_pct = proc.cpu_percent(interval=0.1)
            except:
                state.status = ProcessStatus.ERROR
        
        state.consecutive_restarts = self.consecutive_failures
        return state
    
    def _select_recovery_action(self, state_hash: str) -> ActionType:
        """NeuralLearnerë¡œ ë³µêµ¬ í–‰ë™ ì„ íƒ"""
        available_actions = [
            ActionType.RESTART_IMMEDIATELY,
            ActionType.CHECK_DEPENDENCIES_FIRST,
            ActionType.WAIT_AND_RETRY,
            ActionType.RESTART_WITH_CLEAN_ENV
        ]
        
        action = self.neural_learner.select_action(state_hash, available_actions)
        self.logger.info(f"ì„ íƒëœ ì•¡ì…˜: {action.value}")
        return action
    
    def _execute_action(self, action: ActionType) -> Tuple[bool, int]:
        """í–‰ë™ ì‹¤í–‰"""
        self.logger.info(f"ì•¡ì…˜ ì‹¤í–‰ ì‹œì‘: {action.value}")
        
        if action == ActionType.RESTART_IMMEDIATELY:
            return self.process_restarter.restart_immediately()
        elif action == ActionType.CHECK_DEPENDENCIES_FIRST:
            return self.process_restarter.check_dependencies_first()
        elif action == ActionType.WAIT_AND_RETRY:
            return self.process_restarter.wait_and_retry()
        elif action == ActionType.RESTART_WITH_CLEAN_ENV:
            return self.process_restarter.restart_with_clean_env()
        else:
            return False, 0
    
    def _learn_from_outcome(
        self,
        state_hash: str,
        action: ActionType,
        reward: float,
        next_state_hash: str,
        recovery_time_ms: int,
        success: bool
    ):
        """ê²°ê³¼ë¡œë¶€í„° í•™ìŠµ"""
        # Q-value ì—…ë°ì´íŠ¸
        available_actions = [a for a in ActionType]
        self.neural_learner.update_q_value(
            state_hash,
            action.value,
            reward,
            next_state_hash,
            available_actions
        )
        
        # í–‰ë™ í†µê³„ ê¸°ë¡
        self.neural_learner.record_action_result(
            action.value,
            success,
            recovery_time_ms
        )
        
        # í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸
        self.quality_scorer.record_attempt(
            success,
            recovery_time_ms,
            self.monitor_interval * 1000
        )
        
        self.logger.info(
            f"í•™ìŠµ ì™„ë£Œ: action={action.value}, reward={reward:.1f}, "
            f"recovery_time={recovery_time_ms}ms, success={success}"
        )
    
    def _monitoring_loop(self):
        """ë©”ì¸ ëª¨ë‹ˆí„°ë§ ë£¨í”„ (5ì´ˆë§ˆë‹¤)"""
        self.logger.info("ğŸš€ Watchdog v2 ì‹œì‘ - ì‹ ê²½í•™ìŠµ ì‹œìŠ¤í…œ í™œì„±í™”")
        
        try:
            while True:
                # 1. ìƒíƒœ ê°ì§€
                self.process_state = self._detect_state()
                state_hash = self.process_state.encode()
                
                self.logger.debug(f"í˜„ì¬ ìƒíƒœ: {self.process_state}")
                
                # 2. í”„ë¡œì„¸ìŠ¤ ë‹¤ìš´ ê°ì§€
                if self.process_state.status == ProcessStatus.DOWN:
                    self.logger.critical(f"ğŸ”´ í”„ë¡œì„¸ìŠ¤ ë‹¤ìš´ ê°ì§€!")
                    
                    # 3. í–‰ë™ ì„ íƒ (NeuralLearner)
                    action = self._select_recovery_action(state_hash)
                    
                    # 4. í–‰ë™ ì‹¤í–‰
                    success, recovery_time_ms = self._execute_action(action)
                    
                    # 5. ë³´ìƒ ê³„ì‚°
                    if not success:
                        self.consecutive_failures += 1
                    else:
                        self.consecutive_failures = 0
                    
                    reward = RewardCalculator.calculate_reward(
                        success,
                        recovery_time_ms,
                        self.consecutive_failures
                    )
                    
                    # 6. ë‹¤ìŒ ìƒíƒœ ê°ì§€
                    time.sleep(0.5)  # ì§§ì€ ëŒ€ê¸°
                    next_state = self._detect_state()
                    next_state_hash = next_state.encode()
                    
                    # 7. í•™ìŠµ
                    self._learn_from_outcome(
                        state_hash,
                        action,
                        reward,
                        next_state_hash,
                        recovery_time_ms,
                        success
                    )
                
                # ëŒ€ê¸°
                time.sleep(self.monitor_interval)
        
        except KeyboardInterrupt:
            self.logger.info("ğŸ‘‹ Watchdog ì¢…ë£Œ")
            self._generate_daily_report()
    
    def _generate_daily_report(self):
        """ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± (WorkExecutor íŒ¨í„´)"""
        quality_score = self.quality_scorer.get_quality_score()
        
        report = {
            'date': datetime.now().strftime('%Y-%m-%d'),
            'timestamp': datetime.now().isoformat(),
            'uptime_seconds': self.quality_scorer.uptime_seconds,
            'total_monitored_seconds': self.quality_scorer.total_monitored_seconds,
            'total_downtime_events': self.quality_scorer.total_attempts,
            'successful_recoveries': self.quality_scorer.success_count,
            'recovery_success_rate': round(
                self.quality_scorer.calculate_recovery_rate() * 100, 2
            ),
            'avg_recovery_time_ms': round(
                self.quality_scorer.total_recovery_time_ms / max(1, self.quality_scorer.total_attempts),
                2
            ),
            'quality_metrics': {
                'recovery_rate': round(self.quality_scorer.calculate_recovery_rate() * 100, 2),
                'efficiency': round(self.quality_scorer.calculate_efficiency() * 100, 2),
                'stability': round(self.quality_scorer.calculate_stability() * 100, 2),
                'final_score': quality_score
            },
            'policy_info': self.neural_learner.get_policy_report()
        }
        
        # ì €ì¥
        report_file = Path("logs/watchdog") / f"{datetime.now().strftime('%Y%m%d')}_daily_report_v2.json"
        report_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"ğŸ“Š ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")
        self.logger.info(f"   í’ˆì§ˆ ì ìˆ˜: {quality_score}/100")
        self.logger.info(f"   ë³µêµ¬ìœ¨: {report['recovery_success_rate']}%")
        self.logger.info(f"   í‰ê·  ë³µêµ¬ì‹œê°„: {report['avg_recovery_time_ms']}ms")
    
    def start(self):
        """Watchdog ì‹œì‘"""
        self._monitoring_loop()


# ============================================================================
# 7. ë©”ì¸ ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    script_path = "shawn_bot_telegram.py"
    venv_path = ".venv_bot"
    
    watchdog = BotWatchdogV2(script_path, venv_path)
    watchdog.start()
