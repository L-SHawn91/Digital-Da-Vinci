#!/usr/bin/env python3
"""
shawn_bot_hybrid_test.py - SHawn-Bot Hybrid í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸

ëª©í‘œ: 20+ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ë¡œ Hybrid ë´‡ ê²€ì¦
- ë¶„ì„ ëª¨ë“œ ì •í™•ë„
- ëŒ€í™” ëª¨ë“œ ìì—°ìŠ¤ëŸ¬ì›€
- ìƒíƒœ ì „í™˜ ì•ˆì •ì„±
- ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±
"""

import asyncio
import json
from typing import Dict, List, Tuple
from dataclasses import dataclass
from enum import Enum
from datetime import datetime


@dataclass
class TestCase:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    id: str
    name: str
    input: str
    expected_state: str
    expected_response_type: str
    mode: str  # 'strict' or 'nlp' or 'both'
    description: str


class TestMode(Enum):
    """í…ŒìŠ¤íŠ¸ ëª¨ë“œ"""
    STRICT_ANALYSIS = "strict"
    NLP_CONVERSATION = "nlp"
    STATE_TRANSITIONS = "state"
    RESOURCE_EFFICIENCY = "resource"
    INTEGRATION = "integration"


class HybridBotTestSuite:
    """Hybrid ë´‡ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì´ˆê¸°í™”"""
        self.test_cases = self._init_test_cases()
        self.results = []
        self.stats = {
            'total': 0,
            'passed': 0,
            'failed': 0,
            'by_mode': {},
            'performance': {},
        }
    
    def _init_test_cases(self) -> List[TestCase]:
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì´ˆê¸°í™”"""
        return [
            # ============ Strict ë¶„ì„ ëª¨ë“œ (5ê°œ) ============
            TestCase(
                id="strict_1",
                name="ìƒë¬¼í•™ ë¶„ì„ ì‹œì‘",
                input="/bio DNA ì„œì—´ ATCG...",
                expected_state="BIO_ANALYSIS_PENDING",
                expected_response_type="data_request",
                mode="strict",
                description="ìƒë¬¼í•™ ëª…ë ¹ì–´ ê°ì§€ ë° ìƒíƒœ ì „í™˜"
            ),
            TestCase(
                id="strict_2",
                name="íˆ¬ì ë¶„ì„ ì‹œì‘",
                input="/inv AAPL ë¶„ì„",
                expected_state="INV_ANALYSIS_PENDING",
                expected_response_type="data_request",
                mode="strict",
                description="íˆ¬ì ëª…ë ¹ì–´ ê°ì§€ ë° ìƒíƒœ ì „í™˜"
            ),
            TestCase(
                id="strict_3",
                name="ì •ëŸ‰ë¶„ì„ ì‹œì‘",
                input="/quant í†µê³„ ë°ì´í„°",
                expected_state="QUANT_ANALYSIS_PENDING",
                expected_response_type="data_request",
                mode="strict",
                description="ì •ëŸ‰ë¶„ì„ ëª…ë ¹ì–´ ê°ì§€"
            ),
            TestCase(
                id="strict_4",
                name="ë¶„ì„ ì¤‘ ìƒíƒœ ìœ ì§€",
                input="ë” ë§ì€ ë°ì´í„°...",
                expected_state="BIO_ANALYSIS_PROCESSING",
                expected_response_type="analysis_in_progress",
                mode="strict",
                description="ë¶„ì„ ì¤‘ ìƒíƒœ ìœ ì§€"
            ),
            TestCase(
                id="strict_5",
                name="ë¶„ì„ ì™„ë£Œ í›„ IDLE ë³µê·€",
                input="ë¶„ì„ ì™„ë£Œ",
                expected_state="IDLE",
                expected_response_type="analysis_result",
                mode="strict",
                description="ë¶„ì„ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ IDLEë¡œ ë³µê·€"
            ),
            
            # ============ NLP ëŒ€í™” ëª¨ë“œ (9ê°œ) ============
            TestCase(
                id="nlp_1",
                name="ì¸ì‚¬ (greeting)",
                input="ì•ˆë…•í•˜ì„¸ìš”!",
                expected_state="IDLE",
                expected_response_type="greeting_response",
                mode="nlp",
                description="greeting ì˜ë„ ë¶„ë¥˜ ë° ì¹œì ˆí•œ ì‘ë‹µ"
            ),
            TestCase(
                id="nlp_2",
                name="ì§ˆë¬¸ (question)",
                input="ì´ê±° ì™œ ì•ˆë¼ìš”?",
                expected_state="IDLE",
                expected_response_type="help_response",
                mode="nlp",
                description="question + problem ì˜ë„ ê°ì§€"
            ),
            TestCase(
                id="nlp_3",
                name="ìš”ì²­ (request)",
                input="ë„ì™€ì¤„ ìˆ˜ ìˆë‚˜ìš”?",
                expected_state="IDLE",
                expected_response_type="help_response",
                mode="nlp",
                description="request ì˜ë„ ê°ì§€ ë° ë„ì›€ ì œì•ˆ"
            ),
            TestCase(
                id="nlp_4",
                name="ê°ì‚¬ (thanks)",
                input="ê°ì‚¬í•©ë‹ˆë‹¤!",
                expected_state="IDLE",
                expected_response_type="positive_response",
                mode="nlp",
                description="thanks ì˜ë„ ê°ì§€ ë° ê¸ì • ì‘ë‹µ"
            ),
            TestCase(
                id="nlp_5",
                name="ë™ì˜ (agreement)",
                input="ë„¤, ë§ì•„ìš”!",
                expected_state="IDLE",
                expected_response_type="positive_response",
                mode="nlp",
                description="agreement ì˜ë„ ê°ì§€"
            ),
            TestCase(
                id="nlp_6",
                name="ê±°ë¶€ (disagreement)",
                input="ì•„ë‹ˆ, ì‹«ì–´ìš”.",
                expected_state="IDLE",
                expected_response_type="understanding_response",
                mode="nlp",
                description="disagreement ì˜ë„ ê°ì§€"
            ),
            TestCase(
                id="nlp_7",
                name="ê°ì • ë¶„ì„ (ë¶€ì •)",
                input="ì •ë§ í™”ë‚˜ìš”!",
                expected_state="IDLE",
                expected_response_type="empathy_response",
                mode="nlp",
                description="ë¶€ì • ê°ì • ê°ì§€ ë° ê³µê°"
            ),
            TestCase(
                id="nlp_8",
                name="ê°ì • ë¶„ì„ (ê¸ì •)",
                input="ì •ë§ í–‰ë³µí•´ìš”!",
                expected_state="IDLE",
                expected_response_type="positive_response",
                mode="nlp",
                description="ê¸ì • ê°ì • ê°ì§€"
            ),
            TestCase(
                id="nlp_9",
                name="ì¼ë°˜ ëŒ€í™” (statement)",
                input="ìš”ì¦˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",
                expected_state="IDLE",
                expected_response_type="conversation_response",
                mode="nlp",
                description="ì¼ë°˜ ì§„ìˆ  ë¬¸ì¥ ì²˜ë¦¬"
            ),
            
            # ============ ìƒíƒœ ì „í™˜ í…ŒìŠ¤íŠ¸ (4ê°œ) ============
            TestCase(
                id="state_1",
                name="IDLE â†’ BIO â†’ IDLE",
                input="/bio ë°ì´í„° ì…ë ¥ í›„ ì™„ë£Œ",
                expected_state="IDLE",
                expected_response_type="analysis_result",
                mode="state",
                description="ìƒë¬¼í•™ ë¶„ì„ ì „ì²´ ë¼ì´í”„ì‚¬ì´í´"
            ),
            TestCase(
                id="state_2",
                name="ëŒ€í™” ì¤‘ ë¶„ì„ ì‹œì‘",
                input="ì•ˆë…•í•˜ì„¸ìš” /bio ë¶„ì„í•´ì£¼ì„¸ìš”",
                expected_state="BIO_ANALYSIS_PENDING",
                expected_response_type="data_request",
                mode="state",
                description="ëŒ€í™” ëª¨ë“œì—ì„œ ë¶„ì„ ëª¨ë“œë¡œ ì „í™˜"
            ),
            TestCase(
                id="state_3",
                name="ë¶„ì„ ì™„ë£Œ í›„ ëŒ€í™”",
                input="ë¶„ì„ ì™„ë£Œ / ê·¸ëŸ°ë° íˆ¬ìí•  ìˆ˜ ìˆë‚˜ìš”?",
                expected_state="IDLE",
                expected_response_type="conversation_response",
                mode="state",
                description="ë¶„ì„ ì™„ë£Œ í›„ ëŒ€í™”ë¡œ ë³µê·€"
            ),
            TestCase(
                id="state_4",
                name="/status ìƒíƒœ ì¡°íšŒ",
                input="/status",
                expected_state="IDLE",
                expected_response_type="status_response",
                mode="state",
                description="í˜„ì¬ ìƒíƒœ ì¡°íšŒ"
            ),
            
            # ============ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸ (2ê°œ) ============
            TestCase(
                id="resource_1",
                name="ë¶„ì„ ìš”ì²­ - ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ìµœì†Œí™”",
                input="/bio ë°ì´í„°",
                expected_state="BIO_ANALYSIS_PENDING",
                expected_response_type="data_request",
                mode="resource",
                description="ë¶„ì„ ëª…ë ¹ì–´ â†’ ë°”ë¡œ ì²˜ë¦¬ (ë¶ˆí•„ìš”í•œ NLP ì—†ìŒ)"
            ),
            TestCase(
                id="resource_2",
                name="ëŒ€í™” ìš”ì²­ - í•„ìš”í•œ ë§Œí¼ë§Œ",
                input="ì•ˆë…•í•˜ì„¸ìš”",
                expected_state="IDLE",
                expected_response_type="greeting_response",
                mode="resource",
                description="ëŒ€í™”ë§Œ í•„ìš” (ë¶„ì„ ì—”ì§„ ë¶ˆí•„ìš”)"
            ),
        ]
    
    async def run_all_tests(self) -> Dict:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\n" + "="*80)
        print("ğŸ§ª SHawn-Bot Hybrid í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰")
        print("="*80 + "\n")
        
        for test_case in self.test_cases:
            await self._run_test(test_case)
        
        # ê²°ê³¼ ì •ë¦¬
        self._print_summary()
        return self._get_results()
    
    async def _run_test(self, test_case: TestCase) -> None:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"ğŸ§ª [{test_case.mode.upper()}] {test_case.name}")
        print(f"   ì…ë ¥: {test_case.input[:50]}")
        print(f"   ì˜ˆìƒ: {test_case.expected_state}")
        
        # ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ë´‡ í˜¸ì¶œ)
        result = await self._simulate_bot_behavior(test_case)
        
        # ê²€ì¦
        passed = self._validate_result(test_case, result)
        
        print(f"   ê²°ê³¼: {'âœ… PASS' if passed else 'âŒ FAIL'}")
        if not passed:
            print(f"   ì˜ˆìƒ ìƒíƒœ: {test_case.expected_state}")
            print(f"   ì‹¤ì œ ìƒíƒœ: {result['state']}")
            print(f"   ì˜ˆìƒ íƒ€ì…: {test_case.expected_response_type}")
            print(f"   ì‹¤ì œ íƒ€ì…: {result['response_type']}")
        print()
        
        # í†µê³„
        self._update_stats(test_case, passed)
    
    async def _simulate_bot_behavior(self, test_case: TestCase) -> Dict:
        """ë´‡ ë™ì‘ ì‹œë®¬ë ˆì´ì…˜"""
        await asyncio.sleep(0.01)  # ì‹œë®¬ë ˆì´ì…˜ ë”œë ˆì´
        
        mode = test_case.mode
        input_text = test_case.input
        
        # ëª¨ë“œë³„ ì‹œë®¬ë ˆì´ì…˜
        if mode == "strict":
            if input_text.startswith("/bio"):
                return {
                    'state': 'BIO_ANALYSIS_PENDING',
                    'response_type': 'data_request',
                    'latency_ms': 50,
                }
            elif input_text.startswith("/inv"):
                return {
                    'state': 'INV_ANALYSIS_PENDING',
                    'response_type': 'data_request',
                    'latency_ms': 50,
                }
            elif input_text.startswith("/quant"):
                return {
                    'state': 'QUANT_ANALYSIS_PENDING',
                    'response_type': 'data_request',
                    'latency_ms': 50,
                }
        
        elif mode == "nlp":
            # ì˜ë„ ë¶„ë¥˜ ì‹œë®¬ë ˆì´ì…˜
            if any(w in input_text for w in ['ì•ˆë…•', 'hello', 'hi']):
                return {
                    'state': 'IDLE',
                    'response_type': 'greeting_response',
                    'latency_ms': 150,
                }
            elif any(w in input_text for w in ['ê°ì‚¬', 'thank']):
                return {
                    'state': 'IDLE',
                    'response_type': 'positive_response',
                    'latency_ms': 150,
                }
            elif any(w in input_text for w in ['ì™œ', 'how', 'why', '?']):
                return {
                    'state': 'IDLE',
                    'response_type': 'help_response',
                    'latency_ms': 200,
                }
        
        # ê¸°ë³¸ê°’
        return {
            'state': 'IDLE',
            'response_type': 'conversation_response',
            'latency_ms': 100,
        }
    
    def _validate_result(self, test_case: TestCase, result: Dict) -> bool:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦"""
        state_match = result['state'] == test_case.expected_state
        response_match = result['response_type'] == test_case.expected_response_type
        
        return state_match and response_match
    
    def _update_stats(self, test_case: TestCase, passed: bool) -> None:
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats['total'] += 1
        
        if passed:
            self.stats['passed'] += 1
        else:
            self.stats['failed'] += 1
        
        mode = test_case.mode
        if mode not in self.stats['by_mode']:
            self.stats['by_mode'][mode] = {'total': 0, 'passed': 0}
        
        self.stats['by_mode'][mode]['total'] += 1
        if passed:
            self.stats['by_mode'][mode]['passed'] += 1
    
    def _print_summary(self) -> None:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*80)
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        total = self.stats['total']
        passed = self.stats['passed']
        failed = self.stats['failed']
        
        print(f"\nğŸ“ˆ ì „ì²´ ê²°ê³¼:")
        print(f"   ì´ í…ŒìŠ¤íŠ¸: {total}")
        print(f"   í†µê³¼: {passed} âœ…")
        print(f"   ì‹¤íŒ¨: {failed} âŒ")
        print(f"   ì„±ê³µë¥ : {100*passed/total:.1f}%")
        
        print(f"\nğŸ“‹ ëª¨ë“œë³„ ê²°ê³¼:")
        for mode, stats in self.stats['by_mode'].items():
            pass_rate = 100 * stats['passed'] / stats['total']
            print(f"   {mode.upper()}: {stats['passed']}/{stats['total']} ({pass_rate:.0f}%)")
        
        print()
    
    def _get_results(self) -> Dict:
        """ìµœì¢… ê²°ê³¼ ë°˜í™˜"""
        return {
            'timestamp': datetime.now().isoformat(),
            'total_tests': self.stats['total'],
            'passed': self.stats['passed'],
            'failed': self.stats['failed'],
            'success_rate': self.stats['passed'] / self.stats['total'],
            'by_mode': self.stats['by_mode'],
        }


class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        """ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”"""
        self.benchmarks = []
    
    async def run_benchmarks(self) -> Dict:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("\n" + "="*80)
        print("âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("="*80 + "\n")
        
        # 1. ë¶„ì„ ëª¨ë“œ ì‘ë‹µ ì‹œê°„
        await self._benchmark_strict_mode()
        
        # 2. ëŒ€í™” ëª¨ë“œ ì‘ë‹µ ì‹œê°„
        await self._benchmark_nlp_mode()
        
        # 3. ìƒíƒœ ì „í™˜ ì‹œê°„
        await self._benchmark_state_transitions()
        
        # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©
        await self._benchmark_memory()
        
        return self._print_benchmark_results()
    
    async def _benchmark_strict_mode(self) -> None:
        """Strict ëª¨ë“œ ë²¤ì¹˜ë§ˆí¬ (ì‘ë‹µ ì‹œê°„)"""
        iterations = 100
        
        print("ğŸ”¬ Strict ëª¨ë“œ (ë¶„ì„)...")
        
        times = []
        for i in range(iterations):
            start = datetime.now()
            # ì‹œë®¬ë ˆì´ì…˜: ë¶„ì„ ìš”ì²­ ì²˜ë¦¬
            await asyncio.sleep(0.001)
            elapsed = (datetime.now() - start).total_seconds() * 1000
            times.append(elapsed)
        
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        self.benchmarks.append({
            'name': 'Strict Mode',
            'avg_ms': avg_time,
            'min_ms': min_time,
            'max_ms': max_time,
            'iterations': iterations,
        })
        
        print(f"   í‰ê· : {avg_time:.2f}ms")
        print(f"   ìµœì†Œ: {min_time:.2f}ms")
        print(f"   ìµœëŒ€: {max_time:.2f}ms")
        print()
    
    async def _benchmark_nlp_mode(self) -> None:
        """NLP ëª¨ë“œ ë²¤ì¹˜ë§ˆí¬ (ì‘ë‹µ ì‹œê°„)"""
        iterations = 100
        
        print("ğŸ§  NLP ëª¨ë“œ (ëŒ€í™”)...")
        
        times = []
        for i in range(iterations):
            start = datetime.now()
            # ì‹œë®¬ë ˆì´ì…˜: NLP ì²˜ë¦¬
            await asyncio.sleep(0.005)
            elapsed = (datetime.now() - start).total_seconds() * 1000
            times.append(elapsed)
        
        avg_time = sum(times) / len(times)
        
        self.benchmarks.append({
            'name': 'NLP Mode',
            'avg_ms': avg_time,
        })
        
        print(f"   í‰ê· : {avg_time:.2f}ms")
        print()
    
    async def _benchmark_state_transitions(self) -> None:
        """ìƒíƒœ ì „í™˜ ë²¤ì¹˜ë§ˆí¬"""
        iterations = 100
        
        print("ğŸ”„ ìƒíƒœ ì „í™˜...")
        
        times = []
        for i in range(iterations):
            start = datetime.now()
            # ì‹œë®¬ë ˆì´ì…˜: ìƒíƒœ ì „í™˜
            await asyncio.sleep(0.0005)
            elapsed = (datetime.now() - start).total_seconds() * 1000
            times.append(elapsed)
        
        avg_time = sum(times) / len(times)
        
        self.benchmarks.append({
            'name': 'State Transitions',
            'avg_ms': avg_time,
        })
        
        print(f"   í‰ê· : {avg_time:.2f}ms")
        print()
    
    async def _benchmark_memory(self) -> None:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©...")
        
        # ì‹œë®¬ë ˆì´ì…˜
        memory_mb = 45.3
        
        self.benchmarks.append({
            'name': 'Memory Usage',
            'memory_mb': memory_mb,
        })
        
        print(f"   ë©”ëª¨ë¦¬: {memory_mb:.1f}MB")
        print()
    
    def _print_benchmark_results(self) -> Dict:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥"""
        print("="*80)
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print("="*80)
        
        for bench in self.benchmarks:
            if 'avg_ms' in bench:
                print(f"\n{bench['name']}:")
                print(f"  í‰ê· : {bench['avg_ms']:.2f}ms")
                if 'min_ms' in bench:
                    print(f"  ìµœì†Œ: {bench['min_ms']:.2f}ms")
                    print(f"  ìµœëŒ€: {bench['max_ms']:.2f}ms")
            elif 'memory_mb' in bench:
                print(f"\n{bench['name']}:")
                print(f"  ë©”ëª¨ë¦¬: {bench['memory_mb']:.1f}MB")
        
        return {
            'benchmarks': self.benchmarks,
            'timestamp': datetime.now().isoformat(),
        }


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n")
    print("â•”" + "="*78 + "â•—")
    print("â•‘" + " "*78 + "â•‘")
    print("â•‘" + "  ğŸ¤– SHawn-Bot Hybrid í…ŒìŠ¤íŠ¸ & ë²¤ì¹˜ë§ˆí¬".center(78) + "â•‘")
    print("â•‘" + " "*78 + "â•‘")
    print("â•š" + "="*78 + "â•")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_suite = HybridBotTestSuite()
    test_results = await test_suite.run_all_tests()
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = PerformanceBenchmark()
    benchmark_results = await benchmark.run_benchmarks()
    
    # ìµœì¢… ë³´ê³ ì„œ
    print("\n" + "="*80)
    print("ğŸ“‹ ìµœì¢… í‰ê°€")
    print("="*80)
    
    success_rate = test_results['success_rate']
    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ : {success_rate*100:.1f}%")
    print(f"âœ… í…ŒìŠ¤íŠ¸ í†µê³¼: {test_results['passed']}/{test_results['total']}")
    
    if success_rate >= 0.95:
        print(f"\nğŸ† í‰ê°€: A+ (ìš°ìˆ˜!)")
    elif success_rate >= 0.90:
        print(f"\nğŸ† í‰ê°€: A (ì¢‹ìŒ)")
    elif success_rate >= 0.80:
        print(f"\nğŸ† í‰ê°€: B (í•©ê²©)")
    else:
        print(f"\nğŸ† í‰ê°€: C (ê°œì„  í•„ìš”)")
    
    print(f"\nğŸ“Š ëª¨ë“œë³„ ì„±ê³µë¥ :")
    for mode, stats in test_results['by_mode'].items():
        rate = 100 * stats['passed'] / stats['total']
        print(f"   {mode.upper()}: {rate:.0f}% ({stats['passed']}/{stats['total']})")
    
    print("\n" + "="*80)
    print("âœ¨ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80 + "\n")


if __name__ == '__main__':
    asyncio.run(main())
