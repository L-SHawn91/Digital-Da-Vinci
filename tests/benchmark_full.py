#!/usr/bin/env python3
"""
ì‹ ê²½ê°€ì†Œì„± v2.5 ì¢…í•© ë²¤ì¹˜ë§ˆí¬ (Whole-Brain Evaluation)
LLM ìƒì„± ì†ë„ + ì„ë² ë”© ê²€ìƒ‰ ì†ë„(Memory Latency) í†µí•© ì¸¡ì •
"""

import asyncio
import httpx
import time
from typing import List, Dict
import statistics

API_BASE = "http://localhost:8000"

# í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ (Layerë³„) - ì„ë² ë”© ë¶€í•˜ í…ŒìŠ¤íŠ¸ í¬í•¨
TEST_SCENARIOS = {
    "L1": [ # Reflexive (Memory Skip)
        "ì•ˆë…•",
        "ë„ˆëŠ” ëˆ„êµ¬ì•¼",
        "1+1ì€?",
    ],
    "L3": [ # Cognitive (Memory Intensive)
        "ì˜¤ê°€ë…¸ì´ë“œ ì—°êµ¬ ë™í–¥ì— ëŒ€í•´ ì•Œë ¤ì¤˜", # ì„ë² ë”© í•„ìš”
        "ìµœê·¼ ìˆ€ë¸Œë ˆì¸ ì•„í‚¤í…ì²˜ ë³€ê²½ì‚¬í•­ì€?", # ì„ë² ë”© í•„ìš”
        "íŒŒì´ì¬ ì½”ë”© ì»¨ë²¤ì…˜ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜"  # ì„ë² ë”© í•„ìš”
    ]
}

async def test_query(client: httpx.AsyncClient, user_id: int, text: str) -> Dict:
    """ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
    start = time.time()
    try:
        response = await client.post(
            f"{API_BASE}/v1/chat",
            json={"user_id": user_id, "text": text},
            timeout=60.0 # ë„‰ë„‰í•˜ê²Œ
        )
        response.raise_for_status()
        data = response.json()
        
        # APIê°€ memory_latencyë¥¼ ë°˜í™˜í•œë‹¤ê³  ê°€ì • (ì‹¤ì œë¡œëŠ” ì•ˆ ì¤„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì „ì²´ ì‹œê°„ì—ì„œ ìœ ì¶”í•˜ê±°ë‚˜ ë¡œê·¸ í™•ì¸ í•„ìš”)
        # í˜„ì¬ API ì‘ë‹µ í¬ë§·: {"response": ..., "provider": ..., "latency_ms": ...}
        # memory_latencyëŠ” ë‚´ë¶€ ë¡œê·¸ë¡œë§Œ ë‚¨ìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” Total Time - Latency(LLM)ë¡œ ê°„ì ‘ ì¶”ì •
        
        llm_latency = data.get("latency_ms", 0)
        total_time = (time.time() - start) * 1000
        overhead = max(0, total_time - llm_latency)
        
        return {
            "text": text,
            "provider": data.get("provider", "Unknown"),
            "llm_latency_ms": llm_latency,
            "total_time_ms": total_time,
            "memory_overhead_ms": overhead, # ì¶”ì •ì¹˜ (Network Latency í¬í•¨ë¨)
            "status": "success"
        }
    except Exception as e:
        return {
            "text": text,
            "provider": "Error",
            "llm_latency_ms": 0,
            "total_time_ms": (time.time() - start) * 1000,
            "status": f"failed: {str(e)}"
        }

async def run_benchmark():
    """Whole-Brain ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("=" * 80)
    print("ğŸ§  D-CNS v5.5 Whole-Brain ë²¤ì¹˜ë§ˆí¬ (LLM + Embedding)")
    print("=" * 80)
    
    async with httpx.AsyncClient() as client:
        # Health Check
        try:
            await client.get(f"{API_BASE}/health", timeout=5.0)
            print(f"âœ… Server Online\n")
        except:
            print(f"âŒ Server Offline. start_bot.sh ì‹¤í–‰ í•„ìš”.")
            return
        
        results_by_layer = {}
        
        for level, queries in TEST_SCENARIOS.items():
            print(f"\n{'='*80}")
            print(f"ğŸ“Š Testing {level} Layer")
            print(f"{'='*80}\n")
            
            results = []
            for i, query in enumerate(queries, 1):
                print(f"[{i}] Q: \"{query}\"")
                result = await test_query(client, user_id=99999, text=query)
                results.append(result)
                
                print(f"  â†’ Provider: {result['provider']}")
                print(f"  â†’ Total Latency: {result['total_time_ms']:.0f}ms")
                print(f"  â†’ LLM Gen Time : {result['llm_latency_ms']:.0f}ms")
                print(f"  â†’ Memory/Net   : {result['memory_overhead_ms']:.0f}ms")
                print(f"  â†’ Status: {result['status']}\n")
                
                await asyncio.sleep(1)
            
            results_by_layer[level] = results
        
        # ì¢…í•© ë¦¬í¬íŠ¸ ì¶œë ¥
        print("\n" + "#" * 80)
        print("ğŸ“‘ ìˆ€-ë´‡ ì‹ ê²½ê°€ì†Œì„± ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸")
        print("#" * 80)
        
        for level, rs in results_by_layer.items():
            success_rs = [r for r in rs if r["status"] == "success"]
            if not success_rs: continue
            
            avg_total = statistics.mean([r["total_time_ms"] for r in success_rs])
            avg_llm = statistics.mean([r["llm_latency_ms"] for r in success_rs])
            avg_mem = statistics.mean([r["memory_overhead_ms"] for r in success_rs])
            providers = set([r["provider"] for r in success_rs])
            
            print(f"\nğŸ§  [{level} ê³„ì¸µ í‰ê°€]")
            print(f"- ì„ íƒëœ ì—”ì§„: {', '.join(providers)}")
            print(f"- í‰ê·  ì „ì²´ ì‘ë‹µ ì†ë„: {avg_total:.0f}ms")
            print(f"  â”œâ”€ LLM ìƒì„± (Thinking): {avg_llm:.0f}ms ({avg_llm/avg_total*100:.1f}%)")
            print(f"  â””â”€ ì„ë² ë”©/ê²€ìƒ‰/ë„¤íŠ¸ì›Œí¬: {avg_mem:.0f}ms ({avg_mem/avg_total*100:.1f}%)")
            
            if level == "L1":
                if avg_mem < 100:
                    print("âœ… L1 ìµœì í™” ì„±ê³µ: ë©”ëª¨ë¦¬ ì¡°íšŒ ìƒëµë¨ (Overhead ìµœì†Œ)")
                else:
                    print("âš ï¸ L1 ìµœì í™” í•„ìš”: ì˜¤ë²„í—¤ë“œê°€ ë†’ìŒ")
            elif level == "L3":
                print(f"â„¹ï¸ ì„ë² ë”© ë¶€í•˜ í™•ì¸: ì•½ {avg_mem:.0f}ms ì†Œìš”ë¨")

if __name__ == "__main__":
    asyncio.run(run_benchmark())
